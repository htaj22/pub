{"cells":[{"cell_type":"markdown","metadata":{"id":"w7bqGvvOHmzE"},"source":["# Fine Tuning Transformer for Summary Generation"]},{"cell_type":"markdown","metadata":{"id":"T-bWjf7nHmzG"},"source":["\n","### Introduction\n","\n","In this tutorial we will be fine tuning a transformer model for **Summarization Task**.\n","In this task a summary of a given article/document is generated when passed through a network. There are 2 types of summary generation mechanisms:\n","\n","1. ***Extractive Summary:*** the network calculates the most important sentences from the article and gets them together to provide the most meaningful information from the article.\n","2. ***Abstractive Summary***: The network creates new sentences to encapsulate maximum gist of the article and generates that as output. The sentences in the summary may or may not be contained in the article.\n","\n","In this tutorial we will be generating ***Abstractive Summary***.\n","\n","#### Flow of the notebook\n","\n","* As with all the tutorials previously, this notebook also follows a easy to follow steps. Making the process of fine tuning and training a Transformers model a straight forward task.\n","* However, unlike the other notebooks, in the tutorial, most of the sections have been created into functions, and they are called from the `main()` in the end of the notebook.\n","* This is done to leverage the [Weights and Biases Service](https://www.wandb.com/) WandB in short.\n","* It is a experiment tracking, parameter optimization and artifact management service. That can be very easily integrated to any of the Deep learning or Machine learning frameworks.\n","\n","The notebook will be divided into separate sections to provide a organized walk through for the process used. This process can be modified for individual use cases. The sections are:\n","\n","1. [Preparing Environment and Importing Libraries](#section01)\n","2. [Preparing the Dataset for data processing: Class](#section02)\n","3. [Fine Tuning the Model: Function](#section03)\n","4. [Validating the Model Performance: Function](#section04)\n","5. [Main Function](#section05)\n","    * [Initializing WandB](#section501)\n","    * [Importing and Pre-Processing the domain data](#section502)\n","    * [Creation of Dataset and Dataloader](#section503)\n","    * [Neural Network and Optimizer](#section504)\n","    * [Training Model and Logging to WandB](#section505)\n","    * [Validation and generation of Summary](#section506)\n","6. [Examples of the Summary Generated from the model](#section06)\n","\n","\n","#### Technical Details\n","\n","This script leverages on multiple tools designed by other teams. Details of the tools used below. Please ensure that these elements are present in your setup to successfully implement this script.\n","\n","- **Data**:\n","\t- We are using the News Summary dataset available at [Kaggle](https://www.kaggle.com/sunnysai12345/news-summary)\n","\t- This dataset is the collection created from Newspapers published in India, extracting, details that are listed below.  We are referring only to the first csv file from the data dump: `news_summary.csv`\n","\t- There are`4514` rows of data.  Where each row has the following data-point:\n","\t\t- **author** : Author of the article\n","\t\t- **date** : Date the article was published\n","\t\t- **headline**: Headline for the published article\n","\t\t- **read_more** : URL for the article to follow online\n","\t\t- **text**: This is the summary of the article\n","\t\t- **ctext**: This is the complete article\n","\n","\n","- **Language Model Used**:\n","    - This notebook uses one of the most recent and novel transformers model ***T5***. [Research Paper](https://arxiv.org/abs/1910.10683)    \n","    - ***T5*** in many ways is one of its kind transformers architecture that not only gives state of the art results in many NLP tasks, but also has a very radical approach to NLP tasks.\n","    - **Text-2-Text** - According to the graphic taken from the T5 paper. All NLP tasks are converted to a **text-to-text** problem. Tasks such as translation, classification, summarization and question answering, all of them are treated as a text-to-text conversion problem, rather than seen as separate unique problem statements.\n","    - **Unified approach for NLP Deep Learning** - Since the task is reflected purely in the text input and output, you can use the same model, objective, training procedure, and decoding process to ANY task. Above framework can be used for any task - show Q&A, summarization, etc.\n","   - We will be taking inputs from the T5 paper to prepare our dataset prior to fine tuning and training.    \n","   - [Documentation for python](https://huggingface.co/transformers/model_doc/t5.html)\n","\n","![**Each NLP problem as a “text-to-text” problem** - input: text, output: text](https://miro.medium.com/max/4006/1*D0J1gNQf8vrrUpKeyD8wPA.png)\n","\n","\n","\n","- Hardware Requirements:\n","\t- Python 3.6 and above\n","\t- Pytorch, Transformers and\n","\t- All the stock Python ML Library\n","\t- GPU enabled setup\n","   \n","\n","- **Script Objective**:\n","\t- The objective of this script is to fine tune ***T5 *** to be able to generate summary, that a close to or better than the actual summary  while ensuring the important information from the article is not lost.\n","\n","---\n","NOTE:\n","We are using the Weights and Biases Tool-set in  this tutorial. The different components will be explained as we go through the article."]},{"cell_type":"markdown","metadata":{"id":"WixhpeXFHmzH"},"source":["<a id='section01'></a>\n","### Preparing Environment and Importing Libraries\n","\n","At this step we will be installing the necessary libraries followed by importing the libraries and modules needed to run our script.\n","We will be installing:\n","* transformers\n","* wandb\n","\n","Libraries imported are:\n","* Pandas\n","* Pytorch\n","* Pytorch Utils for Dataset and Dataloader\n","* Transformers\n","* T5 Model and Tokenizer\n","* wandb\n","\n","Followed by that we will preapre the device for CUDA execeution. This configuration is needed if you want to leverage on onboard GPU. First we will check the GPU avaiable to us, using the nvidia command followed by defining our device.\n","\n","Finally, we will be logging into the [wandb](https://www.wandb.com/) serice using the login command"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WD_vnyLXZQzD"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install transformers -q\n","!pip install wandb -q\n","\n","# Code for TPU packages install\n","# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n","# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10410,"status":"ok","timestamp":1690039690064,"user":{"displayName":"Hamed","userId":"13083047042047850989"},"user_tz":240},"id":"TmCeZFE_NuR0","outputId":"65aa6786-b9e5-4fe1-d06f-d9d5588cf454"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.1.99)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install sentencepiece\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"AMQz11JXOcR1"},"outputs":[],"source":["import sentencepiece"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"pzM1_ykHaFur"},"outputs":[],"source":["# Importing stock libraries\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Importing the T5 modules from huggingface/transformers\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","# WandB – Import the wandb library\n","import wandb"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":385,"status":"ok","timestamp":1690040017468,"user":{"displayName":"Hamed","userId":"13083047042047850989"},"user_tz":240},"id":"KvPxXdKJguYB","outputId":"0a1f5efd-f100-4626-edc2-914c3eba53f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["zsh:1: command not found: nvidia-smi\n"]}],"source":["# Checking out the GPU we have access to. This is output is from the google colab version.\n","!nvidia-smi"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NLxxwd1scQNv"},"outputs":[],"source":["# # Setting up the device for GPU usage\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","\n","# Preparing for TPU usage\n","# import torch_xla\n","# import torch_xla.core.xla_model as xm\n","# device = xm.xla_device()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2597,"status":"ok","timestamp":1690040026974,"user":{"displayName":"Hamed","userId":"13083047042047850989"},"user_tz":240},"id":"L-ePh9dEKXMw","outputId":"5ce6635a-a21d-4f24-c710-f7ab4910722f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/pepperhome/.netrc\n"]}],"source":["# Login to wandb to log the model run and all the parameters\n","!wandb login ee4907d763a4597ffac27fd570e8489a92f2cf5e"]},{"cell_type":"markdown","metadata":{"id":"Q5WgYKl4HmzJ"},"source":["<a id='section02'></a>\n","### Preparing the Dataset for data processing: Class\n","\n","We will start with creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. This dataset will be used the the Dataloader method that will feed  the data in batches to the neural network for suitable training and processing.\n","The Dataloader and Dataset will be used inside the `main()`.\n","Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n","\n","#### *CustomDataset* Dataset Class\n","- This class is defined to accept the Dataframe as input and generate tokenized output that is used by the **T5** model for training.\n","- We are using the **T5** tokenizer to tokenize the data in the `text` and `ctext` column of the dataframe.\n","- The tokenizer uses the ` batch_encode_plus` method to perform tokenization and generate the necessary outputs, namely: `source_id`, `source_mask` from the actual text and `target_id` and `target_mask` from the summary text.\n","- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer)\n","- The *CustomDataset* class is used to create 2 datasets, for training and for validation.\n","- *Training Dataset* is used to fine tune the model: **80% of the original data**\n","- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training.\n","\n","#### Dataloader: Called inside the `main()`\n","- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of data loaded to the memory and then passed to the neural network needs to be controlled.\n","- This control is achieved using the parameters such as `batch_size` and `max_len`.\n","- Training and Validation dataloaders are used in the training and validation part of the flow respectively"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"932p8NhxeNw4"},"outputs":[],"source":["# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n","\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.source_len = source_len\n","        self.summ_len = summ_len\n","        self.text = self.data.text\n","        self.ctext = self.data.ctext\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        ctext = str(self.ctext[index])\n","        ctext = ' '.join(ctext.split())\n","\n","        text = str(self.text[index])\n","        text = ' '.join(text.split())\n","\n","        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long),\n","            'source_mask': source_mask.to(dtype=torch.long),\n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }"]},{"cell_type":"markdown","metadata":{"id":"LFAZgVB8HmzK"},"source":["<a id='section03'></a>\n","### Fine Tuning the Model: Function\n","\n","Here we define a training function that trains the model on the training dataset created above, specified number of times (EPOCH), An epoch defines how many times the complete data will be passed through the network.\n","\n","This function is called in the `main()`\n","\n","Following events happen in this function to fine tune the neural network:\n","- The epoch, tokenizer, model, device details, testing_ dataloader and optimizer are passed to the `train ()` when its called from the `main()`\n","- The dataloader passes data to the model based on the batch size.\n","- `language_model_labels` are calculated from the `target_ids` also, `source_id` and `attention_mask` are extracted.\n","- The model outputs first element gives the loss for the forward pass.\n","- Loss value is used to optimize the weights of the neurons in the network.\n","- After every 10 steps the loss value is logged in the wandb service. This log is then used to generate graphs for analysis. Such as [these](https://app.wandb.ai/abhimishra-91/transformers_tutorials_summarization?workspace=user-abhimishra-91)\n","- After every 500 steps the loss value is printed in the console."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":159,"status":"ok","timestamp":1690041673601,"user":{"displayName":"Hamed","userId":"13083047042047850989"},"user_tz":240},"id":"SaPAR7TWmxoM"},"outputs":[],"source":["# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n","# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network\n","\n","def train(epoch, tokenizer, model, device, loader, optimizer):\n","    model.train()\n","    for _,data in enumerate(loader, 0):\n","        y = data['target_ids'].to(device, dtype = torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype = torch.long)\n","        mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","        #outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n","        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n","\n","        loss = outputs[0]\n","\n","        if _%10 == 0:\n","            wandb.log({\"Training Loss\": loss.item()})\n","\n","        if _%500==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # xm.optimizer_step(optimizer)\n","        # xm.mark_step()"]},{"cell_type":"markdown","metadata":{"id":"hn2FmdRTHmzK"},"source":["<a id='section04'></a>\n","### Validating the Model Performance: Function\n","\n","During the validation stage we pass the unseen data(Testing Dataset), trained model, tokenizer and device details to the function to perform the validation run. This step generates new summary for dataset that it has not seen during the training session.\n","\n","This function is called in the `main()`\n","\n","This unseen data is the 20% of `news_summary.csv` which was seperated during the Dataset creation stage.\n","During the validation stage the weights of the model are not updated. We use the generate method for generating new text for the summary.\n","\n","It depends on the `Beam-Search coding` method developed for sequence generation for models with LM head.\n","\n","The generated text and originally summary are decoded from tokens to text and returned to the `main()`"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":136,"status":"ok","timestamp":1690041676963,"user":{"displayName":"Hamed","userId":"13083047042047850989"},"user_tz":240},"id":"j9TNdHlQ0CLz"},"outputs":[],"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask,\n","                #max_length=150,\n","                max_length=30,\n","                num_beams=2,\n","                repetition_penalty=2.5,\n","                length_penalty=1.0,\n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def concat_fields(row):\n","    if row['Sub-issue'] == 'None':\n","        return row['Issue']\n","    else:\n","        return row['Issue'] + ', ' + row['Sub-issue']\n","\n","read_df = pd.read_csv('data/complaints1.csv',encoding='latin-1')\n","read_df['text'] = read_df.apply(concat_fields, axis=1)\n","read_df = read_df.rename(columns={'Consumer complaint narrative': 'ctext'})\n","\n","\n","df = read_df[['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue', 'text', 'ctext']]\n","\n","\n","df.to_csv('data/complaints1_prepped.csv', index=False)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/k9/w33jch99031byvrz4c83qqgm0000gn/T/ipykernel_43449/2577752228.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['text_length'] = df['text'].apply(lambda x: len(str(x)))\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame and 'ctext' is your text column\n","# df = pd.DataFrame(...)\n","\n","# Calculate the length of each text in the 'ctext' column\n","df['text_length'] = df['text'].apply(lambda x: len(str(x)))\n","\n","# Plot the distribution of text lengths\n","plt.hist(df['text_length'], bins='auto') # 'auto' allows matplotlib to choose the number of bins optimally\n","plt.title('Distribution of Summary Text Lengths')\n","plt.xlabel('Summary Text Length')\n","plt.ylabel('Frequency')\n","plt.show()\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/k9/w33jch99031byvrz4c83qqgm0000gn/T/ipykernel_43449/1972321798.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  pd.set_option('display.max_colwidth', -1)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>text_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>658</th>\n","      <td>Problem with a credit reporting company's investigation into an existing problem, Their investigation did not fix an error on your report</td>\n","      <td>137</td>\n","    </tr>\n","    <tr>\n","      <th>165</th>\n","      <td>Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount</td>\n","      <td>136</td>\n","    </tr>\n","    <tr>\n","      <th>1249</th>\n","      <td>Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount</td>\n","      <td>136</td>\n","    </tr>\n","    <tr>\n","      <th>1581</th>\n","      <td>Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount</td>\n","      <td>136</td>\n","    </tr>\n","    <tr>\n","      <th>1451</th>\n","      <td>Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount</td>\n","      <td>136</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                           text  \\\n","658   Problem with a credit reporting company's investigation into an existing problem, Their investigation did not fix an error on your report   \n","165   Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount    \n","1249  Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount    \n","1581  Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount    \n","1451  Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount    \n","\n","      text_length  \n","658   137          \n","165   136          \n","1249  136          \n","1581  136          \n","1451  136          "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Sort the DataFrame by 'text_length' in descending order and select the top 5 rows\n","top_5_rows = df.sort_values('text_length', ascending=False).head(5)\n","\n","pd.set_option('display.max_colwidth', -1)\n","top_5_rows[['text', 'text_length']]"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/k9/w33jch99031byvrz4c83qqgm0000gn/T/ipykernel_43449/1669417454.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['num_tokens'] = df['text'].apply(lambda x: len(str(x).split()))\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame and 'ctext' is your text column\n","# df = pd.DataFrame(...)\n","\n","# Calculate the number of tokens in each text in the 'ctext' column\n","df['num_tokens'] = df['text'].apply(lambda x: len(str(x).split()))\n","\n","# Plot the distribution of text lengths\n","plt.hist(df['num_tokens'], bins='auto') # 'auto' allows matplotlib to choose the number of bins optimally\n","plt.title('Distribution of Summary Text Tokens')\n","plt.xlabel('Summary Text Tokens')\n","plt.ylabel('Frequency')\n","plt.show()\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/k9/w33jch99031byvrz4c83qqgm0000gn/T/ipykernel_43449/2088138088.py:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['num_tokens'] = df['text'].apply(lambda x: len(str(x).split()))\n","/var/folders/k9/w33jch99031byvrz4c83qqgm0000gn/T/ipykernel_43449/2088138088.py:11: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  pd.set_option('display.max_colwidth', -1)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>num_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1451</th>\n","      <td>Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>969</th>\n","      <td>Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>1789</th>\n","      <td>Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>155</th>\n","      <td>Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>960</th>\n","      <td>Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount</td>\n","      <td>25</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                          text  \\\n","1451  Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount   \n","969   Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount   \n","1789  Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount   \n","155   Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount   \n","960   Problem with a lender or other company charging your account, Money was taken from your account on the wrong day or for the wrong amount   \n","\n","      num_tokens  \n","1451  25          \n","969   25          \n","1789  25          \n","155   25          \n","960   25          "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Sort the DataFrame by 'num_tokens' in descending order and select the top 5 rows\n","top_5_rows = df.sort_values('num_tokens', ascending=False).head(5)\n","\n","# Set the maximum column width to a large value to display the full text\n","pd.set_option('display.max_colwidth', -1)\n","\n","# Display 'ctext' and 'num_tokens' columns from the top 5 rows\n","top_5_rows[['text', 'num_tokens']]\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date received</th>\n","      <th>Product</th>\n","      <th>Sub-product</th>\n","      <th>Issue</th>\n","      <th>Sub-issue</th>\n","      <th>text</th>\n","      <th>ctext</th>\n","      <th>text_length</th>\n","      <th>num_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3/1/23</td>\n","      <td>Checking or savings account</td>\n","      <td>Checking account</td>\n","      <td>Problem caused by your funds being low</td>\n","      <td>Overdrafts and overdraft fees</td>\n","      <td>Problem caused by your funds being low, Overdrafts and overdraft fees</td>\n","      <td>I stopped using my wells Fargo account because Im XXXX they started to take out money even though I didnt have any transactions linked to that account I switched to XXXX XXXX XXXX all my bills moved to that one but money was still being taken out an charged to the wells Fargo account all the way up to now I just recently closed the account but it was negative {$860.00} an my savings was negative {$64.00} because of the over drafts which were scam transactions.</td>\n","      <td>69</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3/1/23</td>\n","      <td>Checking or savings account</td>\n","      <td>Other banking product or service</td>\n","      <td>Managing an account</td>\n","      <td>Problem making or receiving payments</td>\n","      <td>Managing an account, Problem making or receiving payments</td>\n","      <td>-On XX/XX/XXXX, I tried to deposit a check in the amount of {$8900.00} through Wells Fargo XXXX XXXX  and received and error message stating that I was over the {$5000.00} daily mobile deposit limit. \\n\\nSo I went to Well Fargo Branch # XXXX XXXX to deposit the check along with requesting {$250.00} cash back from transaction. The teller gave me my cash and advised that there would be a temporary hold on the check but I will have access to all of the funds XX/XX/XXXX. \\n\\nI received a printed receipt showing that {$8700.00} would be delayed XXXX business days and that the date funds would be available is XX/XX/XXXX. \\n\\nXXXX XX/XX/XXXX I received an alert through Wells Fargo XXXX XXXX XXXX XXXX Deposit Hold Alert Deposit Date XX/XX/XXXX Deposit Amount : {$8700.00} Amount Delayed : {$8700.00} Hold Reason : Unusual Deposit Account Activity Date Funds Will Be Available : XX/XX/XXXX -As of XX/XX/XXXX, I still do not have access to my funds. I contacted Wells Fargo and was advised by ( XXXX ) supervisors that XXXX  XXXX XXXX was holding up funds. \\nThen a second supervisor stated that the issue was with the company that issued payment. Which was flat out lies. \\n\\nXXXX  XXXX XXXX and the party that issued payment confirmed confirmation that the check cleared. They also provided the BOA Sequence # XXXX along with WF transaction/sequence # as XXXX. \\n\\nThis has caused a domino effect my transactions and use of my checking account in an attempt to cause me to incur overdraft fees. Wells Fargo is being deceptive in their business practices and illegally withholding my funds.</td>\n","      <td>57</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3/1/23</td>\n","      <td>Mortgage</td>\n","      <td>FHA mortgage</td>\n","      <td>Trouble during payment process</td>\n","      <td>None</td>\n","      <td>Trouble during payment process</td>\n","      <td>This is to inform this office that Wells Fargo has lied to this office in its response. \\n\\nI am referencing the response that states \" ... we have been unable to contact XXXX XXXX ''. \\n\\nThat is a FRAUD. I have called on seven occasions. I have left messages. I have emailed. I was told by \" XXXX '' who left a message I would have an answer by XX/XX/XXXX. I was told five times I would receive a call in 24 hours. That was weeks ago. \\n\\nThis is abuse upon abuse. \\n\\nMy equity was STOLEN my life destroyed. My health destroyed. \\n\\nI am demanding the CFPB enforce the order and force repayment of all damages and further punish them for LYING To this body. \\n\\nHOW CAN YOU ALLOW WELLS FARGO TO FURTHER ANUSE AND LIE AND STEAL?</td>\n","      <td>30</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Date received                      Product  \\\n","0  3/1/23        Checking or savings account   \n","1  3/1/23        Checking or savings account   \n","2  3/1/23        Mortgage                      \n","\n","                        Sub-product                                   Issue  \\\n","0  Checking account                  Problem caused by your funds being low   \n","1  Other banking product or service  Managing an account                      \n","2  FHA mortgage                      Trouble during payment process           \n","\n","                              Sub-issue  \\\n","0  Overdrafts and overdraft fees          \n","1  Problem making or receiving payments   \n","2  None                                   \n","\n","                                                                    text  \\\n","0  Problem caused by your funds being low, Overdrafts and overdraft fees   \n","1  Managing an account, Problem making or receiving payments               \n","2  Trouble during payment process                                          \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ctext  \\\n","0  I stopped using my wells Fargo account because Im XXXX they started to take out money even though I didnt have any transactions linked to that account I switched to XXXX XXXX XXXX all my bills moved to that one but money was still being taken out an charged to the wells Fargo account all the way up to now I just recently closed the account but it was negative {$860.00} an my savings was negative {$64.00} because of the over drafts which were scam transactions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n","1  -On XX/XX/XXXX, I tried to deposit a check in the amount of {$8900.00} through Wells Fargo XXXX XXXX  and received and error message stating that I was over the {$5000.00} daily mobile deposit limit. \\n\\nSo I went to Well Fargo Branch # XXXX XXXX to deposit the check along with requesting {$250.00} cash back from transaction. The teller gave me my cash and advised that there would be a temporary hold on the check but I will have access to all of the funds XX/XX/XXXX. \\n\\nI received a printed receipt showing that {$8700.00} would be delayed XXXX business days and that the date funds would be available is XX/XX/XXXX. \\n\\nXXXX XX/XX/XXXX I received an alert through Wells Fargo XXXX XXXX XXXX XXXX Deposit Hold Alert Deposit Date XX/XX/XXXX Deposit Amount : {$8700.00} Amount Delayed : {$8700.00} Hold Reason : Unusual Deposit Account Activity Date Funds Will Be Available : XX/XX/XXXX -As of XX/XX/XXXX, I still do not have access to my funds. I contacted Wells Fargo and was advised by ( XXXX ) supervisors that XXXX  XXXX XXXX was holding up funds. \\nThen a second supervisor stated that the issue was with the company that issued payment. Which was flat out lies. \\n\\nXXXX  XXXX XXXX and the party that issued payment confirmed confirmation that the check cleared. They also provided the BOA Sequence # XXXX along with WF transaction/sequence # as XXXX. \\n\\nThis has caused a domino effect my transactions and use of my checking account in an attempt to cause me to incur overdraft fees. Wells Fargo is being deceptive in their business practices and illegally withholding my funds.   \n","2  This is to inform this office that Wells Fargo has lied to this office in its response. \\n\\nI am referencing the response that states \" ... we have been unable to contact XXXX XXXX ''. \\n\\nThat is a FRAUD. I have called on seven occasions. I have left messages. I have emailed. I was told by \" XXXX '' who left a message I would have an answer by XX/XX/XXXX. I was told five times I would receive a call in 24 hours. That was weeks ago. \\n\\nThis is abuse upon abuse. \\n\\nMy equity was STOLEN my life destroyed. My health destroyed. \\n\\nI am demanding the CFPB enforce the order and force repayment of all damages and further punish them for LYING To this body. \\n\\nHOW CAN YOU ALLOW WELLS FARGO TO FURTHER ANUSE AND LIE AND STEAL?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n","\n","   text_length  num_tokens  \n","0  69           11          \n","1  57           8           \n","2  30           4           "]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["df.head(3)"]},{"cell_type":"markdown","metadata":{"id":"cYBcHwvvHmzK"},"source":["<a id='section05'></a>\n","### Main Function\n","\n","The `main()` as the name suggests is the central location to execute all the functions/flows created above in the notebook. The following steps are executed in the `main()`:\n","\n","\n","<a id='section501'></a>\n","#### Initializing WandB\n","\n","* The `main()` begins with initializing WandB run under a specific project. This command initiates a new run for each execution of this command.\n","\n","* Before we proceed any further i will give a brief overview of the **[WandB Service](https://www.wandb.com/)**\n","\n","* This service has been created to track ML experiments, Optimize the experiments and save artifacts. It is designed to seamlessly integrate with all the Machine Learning and Deep Learning Frameworks. Each script can be organized into *Project* and each execution of the script will be registered as a *run* in the respective project.\n","\n","* The service can be configured to log several default metrics, such a network weights, hardware usage, gradients and weights of the network.\n","\n","* It can also be used to log user defined metrics, such a loss in the `train()`.\n","\n","* This particular tutorial is logged in the project: **[transformers_tutorials_summarization](https://app.wandb.ai/abhimishra-91/transformers_tutorials_summarization?workspace=user-abhimishra-91)**\n","\n","**One of the charts from the project**\n","![](https://github.com/abhimishra91/transformers-tutorials/blob/master/meta/wandb.png?raw=1)\n","\n","* Visit the project page to see the details of different runs and what information is logged by the service.\n","\n","* Following the initialization of the WandB service we define configuration parameters that will be used across the tutorial such as `batch_size`, `epoch`, `learning_rate` etc.\n","\n","* These parameters are also passed to the WandB config. The config construct with all the parameters can be optimized using the Sweep service from WandB. Currently, that is outof scope of this tutorial.\n","\n","* Next we defining seed values so that the experiment and results can be reproduced.\n","\n","\n","<a id='section502'></a>\n","#### Importing and Pre-Processing the domain data\n","\n","We will be working with the data and preparing it for fine tuning purposes.\n","*Assuming that the `news_summary.csv` is already downloaded in your `data` folder*\n","\n","* The file is imported as a dataframe and give it the headers as per the documentation.\n","* Cleaning the file to remove the unwanted columns.\n","* A new string is added to the main article column `summarize: ` prior to the actual article. This is done because **T5** had similar formatting for the summarization dataset.\n","* The final Dataframe will be something like this:\n","\n","|text|ctext|\n","|--|--|\n","|summary-1|summarize: article 1|\n","|summary-2|summarize: article 2|\n","|summary-3|summarize: article 3|\n","\n","* Top 5 rows of the dataframe are printed on the console.\n","\n","<a id='section503'></a>\n","#### Creation of Dataset and Dataloader\n","\n","* The updated dataframe is divided into 80-20 ratio for test and validation.\n","* Both the data-frames are passed to the `CustomerDataset` class for tokenization of the new articles and their summaries.\n","* The tokenization is done using the length parameters passed to the class.\n","* Train and Validation parameters are defined and passed to the `pytorch Dataloader contstruct` to create `train` and `validation` data loaders.\n","* These dataloaders will be passed to `train()` and `validate()` respectively for training and validation action.\n","* The shape of datasets is printed in the console.\n","\n","\n","<a id='section504'></a>\n","#### Neural Network and Optimizer\n","\n","* In this stage we define the model and optimizer that will be used for training and to update the weights of the network.\n","* We are using the `t5-base` transformer model for our project. You can read about the `T5 model` and its features above.\n","* We use the `T5ForConditionalGeneration.from_pretrained(\"t5-base\")` commad to define our model. The `T5ForConditionalGeneration` adds a Language Model head to our `T5 model`. The Language Model head allows us to generate text based on the training of `T5 model`.\n","* We are using the `Adam` optimizer for our project. This has been a standard for all our tutorials and is something that can be changed updated to see how different optimizer perform with different learning rates.\n","* There is also a scope for doing more with Optimizer such a decay, momentum to dynamically update the Learning rate and other parameters. All those concepts have been kept out of scope for these tutorials.\n","\n","\n","<a id='section505'></a>\n","#### Training Model and Logging to WandB\n","\n","* Now we log all the metrics in WandB project that we have initialized above.\n","* Followed by that we call the `train()` with all the necessary parameters.\n","* Loss at every 500th step is printed on the console.\n","* Loss at every 10th step is logged as Loss in the WandB service.\n","\n","\n","<a id='section506'></a>\n","#### Validation and generation of Summary\n","\n","* After the training is completed, the validation step is initiated.\n","* As defined in the validation function, the model weights are not updated. We use the fine tuned model to generate new summaries based on the article text.\n","* An output is printed on the console giving a count of how many steps are complete after every 100th step.\n","* The original summary and generated summary are converted into a list and returned to the main function.\n","* Both the lists are used to create the final dataframe with 2 columns **Generated Summary** and **Actual Summary**\n","* The dataframe is saved as a csv file in the local drive.\n","* A qualitative analysis can be done with the Dataframe."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e1d182d7487342809470665d39359c3a","4bd9b972524d403fbc356be68b9d188e","7aa920950cd343b780fb14b89f0e791a","20326ba4a1ac454e9ef7e5db0d08e0ec","bff151e590184f5aaed5a034919294ea","c9edd3ead5714b01864b32fc3a4283d5","bffde4b3a08b4fecb6ca73027fe800a2","c446e2d887ac468db6381b5cb905f7fa","0db34048ca4c4563ab073cf78d4d04a8","6beb76bc3adf49cb90eeb145d2e9b85a","f04b3e727b8c4869b844e429cbc9ec47","35dcf351426a4a569783c64e2c2afaab","6b802411f9e5449ba756d696d6e5293c","43a0d0df745e453383259065bc7d76c4","c68bbd49e1f746fb84e1ab665abbb169","6f156748cc37450d9c5312952a86901a"]},"id":"ZtNs9ytpCow2","outputId":"c679c6a7-fce8-44df-8723-5e9e9702bdf8"},"outputs":[{"data":{"text/html":["Finishing last run (ID:9f88u8v6) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">rose-resonance-1</strong> at: <a href='https://wandb.ai/team-taj/transformers_t5_summarization_capstone/runs/9f88u8v6' target=\"_blank\">https://wandb.ai/team-taj/transformers_t5_summarization_capstone/runs/9f88u8v6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230723_102840-9f88u8v6/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:9f88u8v6). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1678f4b333e8422fb03b6fcea7f3b9a3","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016697543050152794, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/pepperhome/Library/Mobile Documents/com~apple~CloudDocs/My_Desk/Uni/MyCode/MyCapstone/Code/wandb/run-20230723_102958-f6cjhxuo</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/team-taj/transformers_t5_summarization_capstone/runs/f6cjhxuo' target=\"_blank\">zany-wildflower-2</a></strong> to <a href='https://wandb.ai/team-taj/transformers_t5_summarization_capstone' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/team-taj/transformers_t5_summarization_capstone' target=\"_blank\">https://wandb.ai/team-taj/transformers_t5_summarization_capstone</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/team-taj/transformers_t5_summarization_capstone/runs/f6cjhxuo' target=\"_blank\">https://wandb.ai/team-taj/transformers_t5_summarization_capstone/runs/f6cjhxuo</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["                                                                    text  \\\n","0  Problem caused by your funds being low, Overdrafts and overdraft fees   \n","1  Managing an account, Problem making or receiving payments               \n","2  Trouble during payment process                                          \n","3  Managing an account, Deposits and withdrawals                           \n","4  Struggling to pay mortgage                                              \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ctext  \n","0  summarize: I stopped using my wells Fargo account because Im XXXX they started to take out money even though I didnt have any transactions linked to that account I switched to XXXX XXXX XXXX all my bills moved to that one but money was still being taken out an charged to the wells Fargo account all the way up to now I just recently closed the account but it was negative {$860.00} an my savings was negative {$64.00} because of the over drafts which were scam transactions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n","1  summarize: -On XX/XX/XXXX, I tried to deposit a check in the amount of {$8900.00} through Wells Fargo XXXX XXXX  and received and error message stating that I was over the {$5000.00} daily mobile deposit limit. \\n\\nSo I went to Well Fargo Branch # XXXX XXXX to deposit the check along with requesting {$250.00} cash back from transaction. The teller gave me my cash and advised that there would be a temporary hold on the check but I will have access to all of the funds XX/XX/XXXX. \\n\\nI received a printed receipt showing that {$8700.00} would be delayed XXXX business days and that the date funds would be available is XX/XX/XXXX. \\n\\nXXXX XX/XX/XXXX I received an alert through Wells Fargo XXXX XXXX XXXX XXXX Deposit Hold Alert Deposit Date XX/XX/XXXX Deposit Amount : {$8700.00} Amount Delayed : {$8700.00} Hold Reason : Unusual Deposit Account Activity Date Funds Will Be Available : XX/XX/XXXX -As of XX/XX/XXXX, I still do not have access to my funds. I contacted Wells Fargo and was advised by ( XXXX ) supervisors that XXXX  XXXX XXXX was holding up funds. \\nThen a second supervisor stated that the issue was with the company that issued payment. Which was flat out lies. \\n\\nXXXX  XXXX XXXX and the party that issued payment confirmed confirmation that the check cleared. They also provided the BOA Sequence # XXXX along with WF transaction/sequence # as XXXX. \\n\\nThis has caused a domino effect my transactions and use of my checking account in an attempt to cause me to incur overdraft fees. Wells Fargo is being deceptive in their business practices and illegally withholding my funds.  \n","2  summarize: This is to inform this office that Wells Fargo has lied to this office in its response. \\n\\nI am referencing the response that states \" ... we have been unable to contact XXXX XXXX ''. \\n\\nThat is a FRAUD. I have called on seven occasions. I have left messages. I have emailed. I was told by \" XXXX '' who left a message I would have an answer by XX/XX/XXXX. I was told five times I would receive a call in 24 hours. That was weeks ago. \\n\\nThis is abuse upon abuse. \\n\\nMy equity was STOLEN my life destroyed. My health destroyed. \\n\\nI am demanding the CFPB enforce the order and force repayment of all damages and further punish them for LYING To this body. \\n\\nHOW CAN YOU ALLOW WELLS FARGO TO FURTHER ANUSE AND LIE AND STEAL?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n","3  summarize: I tried to deposit a tax refund check from the internal revenue service on XX/XX/XXXX at the ATM at my local Wells Fargo branch in XXXX XXXX, VA. The refund was long overdue and was in the name of the trust established by my now deceased mother and father, the XXXX Trust. The Trust had an account established at that Wells Fargo for more than a year. The deposit was in the amount of {$22000.00} so it was already difficult that it had taken the IRS years to process and provide the refund check ( long enough that my mother passed away in the interim ). \\n\\nWhen I deposited the check, I was presented with a message telling me that Wells Fargo would not deposit the check and would not return the check to me. They may as well have taken it from my hands and run away. After calling their customer service number and waiting on hold ( and being late for work ), they told me that all I could do was file a complaint and wait 10 days. \\n\\nApparently, I need to wait that long to find out whether the bank will steal my deceased mother 's money or not. I can't believe this is allowed to happen. The check was from the U.S. Internal Revenue Service- not a shady, offshore organization. I don't understand how they can keep that large sum of money with no justification or explanation. It is a lot of money to need to worry may not be returned. If an individual did this, the authorities would take them into custody.                                                                                                                                                                                \n","4  summarize: I believe Wells Fargo fraudulent created and implemented a false Forbearance agreement on my mortgage then sold the mortgage to a servicing company.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n","FULL Dataset: (1918, 2)\n","TRAIN Dataset: (1534, 2)\n","TEST Dataset: (384, 2)\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Initiating Fine-Tuning for the model on our dataset\n"]},{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss:  11.269132614135742\n","Epoch: 0, Loss:  0.21682488918304443\n","Epoch: 1, Loss:  0.216889426112175\n","Epoch: 1, Loss:  0.16202887892723083\n","Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n","Completed 0\n"]},{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Completed 100\n","Output Files generated for review\n"]}],"source":["def main():\n","    # WandB – Initialize a new run\n","    wandb.init(project=\"transformers_t5_summarization_capstone\")\n","\n","    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n","    # Defining some key variables that will be used later on in the training\n","    config = wandb.config          # Initialize config\n","    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n","    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n","    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n","    config.VAL_EPOCHS = 1\n","    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","    config.SEED = 42               # random seed (default: 42)\n","    config.MAX_LEN = 512\n","    # config.SUMMARY_LEN = 150\n","    config.SUMMARY_LEN = 50\n","\n","    # Set random seeds and deterministic pytorch for reproducibility\n","    torch.manual_seed(config.SEED) # pytorch random seed\n","    np.random.seed(config.SEED) # numpy random seed\n","    torch.backends.cudnn.deterministic = True\n","\n","    # tokenzier for encoding the text\n","    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","\n","\n","    # Importing and Pre-Processing the domain data\n","    # Selecting the needed columns only.\n","    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task.\n","    #df = pd.read_csv('./data/news_summary.csv',encoding='latin-1')\n","    df = pd.read_csv('data/complaints1_prepped.csv',encoding='latin-1')\n","\n","    df = df[['text','ctext']]\n","    df.ctext = 'summarize: ' + df.ctext\n","    print(df.head())\n","\n","\n","    # Creation of Dataset and Dataloader\n","    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation.\n","    train_size = 0.8\n","    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n","    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","    train_dataset = train_dataset.reset_index(drop=True)\n","\n","    print(\"FULL Dataset: {}\".format(df.shape))\n","    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n","\n","\n","    # Creating the Training and Validation dataset for further creation of Dataloader\n","    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","\n","    # Defining the parameters for creation of dataloaders\n","    train_params = {\n","        'batch_size': config.TRAIN_BATCH_SIZE,\n","        'shuffle': True,\n","        'num_workers': 0\n","        }\n","\n","    val_params = {\n","        'batch_size': config.VALID_BATCH_SIZE,\n","        'shuffle': False,\n","        'num_workers': 0\n","        }\n","\n","    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","    training_loader = DataLoader(training_set, **train_params)\n","    val_loader = DataLoader(val_set, **val_params)\n","\n","\n","\n","    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n","    # Further this model is sent to device (GPU/TPU) for using the hardware.\n","    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n","    model = model.to(device)\n","\n","    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n","    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n","\n","    # Log metrics with wandb\n","    wandb.watch(model, log=\"all\")\n","    # Training loop\n","    print('Initiating Fine-Tuning for the model on our dataset')\n","\n","    for epoch in range(config.TRAIN_EPOCHS):\n","        train(epoch, tokenizer, model, device, training_loader, optimizer)\n","\n","\n","    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n","    # Saving the dataframe as predictions.csv\n","    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","    for epoch in range(config.VAL_EPOCHS):\n","        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","        #final_df.to_csv('./models/predictions.csv')\n","        final_df.to_csv('models/complaints2_predictions.csv')\n","        print('Output Files generated for review')\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":156,"status":"ok","timestamp":1690040735888,"user":{"displayName":"Hamed","userId":"13083047042047850989"},"user_tz":240},"id":"31bDBQ1YQBkF","outputId":"6efd18dd-e201-49ae-de8d-7e7d72c74c22"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["import os\n","print(os.getcwd())"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":852,"status":"ok","timestamp":1690040738919,"user":{"displayName":"Hamed","userId":"13083047042047850989"},"user_tz":240},"id":"_JEELG4CQvsN","outputId":"2a1b1a09-c3a4-4d75-af99-22286170c1da"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","metadata":{"id":"RkKQnVO2HmzL"},"source":["<a id='section06'></a>\n","### Examples of the Summary Generated from the model\n","\n","##### Example 1\n","\n","**Original Text**\n","New Delhi, Apr 25 (PTI) Union minister Vijay Goel today batted for the unification of the three municipal corporations in the national capital saying a discussion over the issue was pertinent. The BJP leader, who was confident of a good show by his party in the MCD polls, the results of which will be declared tomorrow, said the civic bodies needed to be \"revamped\" in order to deliver the services to the people more effectively. The first thing needed was a discussion on the unification of the three municipal corporations and there should also be an end to the practice of sending Delhi government officials to serve in the civic bodies, said the Union Minister of State (Independent Charge) for Youth Affairs and Sports. \"Barring one, the two other civic bodies have been incurring losses. It would be more fruitful and efficient if all the three were merged,\" he said, referring to the north, south and east Delhi municipal corporations. The erstwhile Municipal Corporation of Delhi (MCD) was trifurcated into NDMC, SDMC and EDMC by the then Sheila Dikshit-led Delhi government in 2012. Goel predicted a \"thumping\" victory for the BJP in the MCD polls. He said the newly-elected BJP councillors will be trained on the functioning of the civic bodies and dealing with the bureaucracy.\n","\n","\n","**Original Summary**\n","Union Minister Vijay Goel has favoured unification of three MCDs ? North, South and East ? in order to deliver the services more effectively. \"Barring one, the two other civic bodies have been incurring losses. It would be more fruitful and efficient if all the three were merged,\" he said. MCD was trifurcated into EDMC, NDMC and SDMC in 2012.\n","\n","**Generated Summary**\n","BJP leader Vijay Goel on Saturday batted for the unification of three municipal corporations in the national capital saying a discussion over this was pertinent. \"Barring one, two other civic bodies have been incurring losses,\" said Goels. The erstwhile Municipal Corporations of Delhi (MCD) were trifurcated into NDMC and SDMC by the then Sheilha Dikshi-led government in 2012. Notably, the MCD poll results will be declared tomorrow."]},{"cell_type":"markdown","metadata":{"id":"UKC8vy-SHmzL"},"source":["##### Example 2\n","\n","**Original Text**\n","After much wait, the first UDAN flight took off from Shimla today after being flagged off by Prime Minister Narendra Modi.The flight will be operated by Alliance Air, the regional arm of Air India. PM Narendra Modi handed over boarding passes to some of passengers travelling via the first UDAN flight at the Shimla airport.Tomorrow PM @narendramodi will flag off the first UDAN flight under the Regional Connectivity Scheme, on Shimla-Delhi sector.Air India yesterday opened bookings for the first launch flight from Shimla to Delhi with all inclusive fares starting at Rs2,036.THE GREAT 'UDAN'The UDAN (Ude Desh ka Aam Naagrik) scheme seeks to make flying more affordable for the common people, holding a plan to connect over 45 unserved and under-served airports.Under UDAN, 50 per cent of the seats on each flight would have a cap of Rs 2,500 per seat/hour. The government has also extended subsidy in the form of viability gap funding to the operators flying on these routes.The scheme was launched to \"make air travel accessible to citizens in regionally important cities,\" and has been described as \"a first-of-its-kind scheme globally to stimulate regional connectivity through a market-based mechanism.\" Report have it the first flight today will not be flying at full capacity on its 70-seater ATR airplane because of payload restrictions related to the short Shimla airfield.|| Read more ||Udan scheme: Now you can fly to these 43 cities, see the full list hereUDAN scheme to fly hour-long flights capped at Rs 2,500 to smaller cities\n","\n","\n","**Original Summary**\n","PM Narendra Modi on Thursday launched Ude Desh ka Aam Nagrik (UDAN) scheme for regional flight connectivity by flagging off the inaugural flight from Shimla to Delhi. Under UDAN, government will connect small towns by air with 50% plane seats' fare capped at?2,500 for a one-hour journey of 500 kilometres. UDAN will connect over 45 unserved and under-served airports.\n","\n","**Generated Summary**\n","UDAN (Ude Desh Ka Aam Naagrik) scheme, launched to make air travel accessible in regionally important cities under the Regional Connectivity Scheme, took off from Shimla on Tuesday. The first flight will be operated by Alliance Air, which is the regional arm of India's Air India. Under the scheme, 50% seats would have?2,500 per seat/hour and 50% of the seats would have capped at this rate. It was also extended subsidy in form-based funding for operators flying these routes as well."]},{"cell_type":"markdown","metadata":{"id":"pLYNwtXVHmzL"},"source":["##### Example 3\n","\n","**Original Text**\n","New Delhi, Apr 25 (PTI) The Income Tax department has issued a Rs 24,646 crore tax demand notice to Sahara Groups Aamby Valley Limited (AVL) after conducting a special audit of the company. The department, as part of a special investigation and audit into the account books of AVL, found that an income of over Rs 48,000 crore for a particular assessment year was allegedly not reflected in the record books of the firm and hence it raised a fresh tax demand and penalty amount on it. A Sahara Group spokesperson confirmed the development to PTI. \"Yes, the Income Tax Department has raised Rs 48,085.79 crores to the income of the Aamby Valley Limited with a total demand of income tax of Rs 24,646.96 crores on the Aamby Valley Limited,\" the spokesperson said in a brief statement. Officials said the notice was issued by the taxman in January this year after the special audit of AVLs income for the Assessment Year 2012-13 found that the parent firm had allegedly floated a clutch of Special Purpose Vehicles whose incomes were later accounted on the account of AVL as they were merged with the former in due course of time. The AVL, in its income return filed for AY 2012-13, had reflected a loss of few crores but the special I-T audit brought up the added income, a senior official said. The Supreme Court, last week, had asked the Bombay High Courts official liquidator to sell the Rs 34,000 crore worth of properties of Aamby Valley owned by the Sahara Group and directed its chief Subrata Roy to personally appear before it on April 28.  \n","\n","\n","**Original Summary**\n","The Income Tax Department has issued a ?24,646 crore tax demand notice to Sahara Group's Aamby Valley Limited. The department's audit found that an income of over ?48,000 crore for the assessment year 2012-13 was not reflected in the record books of the firm. A week ago, the SC ordered Bombay HC to auction Sahara's Aamby Valley worth ?34,000 crore.\n","\n","**Generated Summary**\n","the Income Tax department has issued a?24,646 crore tax demand notice to Sahara Groups Aamby Valley Limited (AVL) after conducting an audit of the company. The notice was issued in January this year after the special audit found that the parent firm had floated Special Purpose Vehicle income for the Assessment Year 2012-13 and later accounted on its account as they were merged with the former. \"Yes...the Income Tax Department raised Rs48,085.79 crores to the income,\" he added earlier said at the notice."]},{"cell_type":"markdown","metadata":{},"source":["# ROGUE Score\n","To measure the quality of the generated summaries, we can use the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric. This is a common evaluation metric for text summarization tasks.\n","\n","The ROUGE metric compares the generated summary with a reference summary (also known as the ground truth or baseline summary) and calculates scores based on the overlap of n-grams, where an n-gram is a contiguous sequence of n words.\n","\n","In Python, you can use the rouge package to calculate ROUGE scores. Here's how you might use it to evaluate your T5 summaries:"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rouge) (1.16.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.1\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install rouge\n","from rouge import Rouge"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["df_output = pd.read_csv('models/complaints1_predictions.csv',encoding='latin-1')"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'rouge-1': {'f': 0.18711176438565336, 'p': 0.21886922352225668, 'r': 0.193181315666743}, 'rouge-2': {'f': 0.09561394638851406, 'p': 0.12728949673108284, 'r': 0.08980882433578367}, 'rouge-l': {'f': 0.17759499130464787, 'p': 0.20935159271556328, 'r': 0.18239337675380415}}\n"]}],"source":["from rouge import Rouge\n","\n","# Assuming df is your DataFrame with 'Summary' and 'Generated_Summary' columns\n","# df = pd.DataFrame(...)\n","\n","rouge = Rouge()\n","\n","scores = []\n","for index, row in df_output.iterrows():\n","    try:\n","        score = rouge.get_scores(row['Generated Text'], row['Actual Text'])[0]\n","        scores.append(score)\n","    except Exception as e:\n","        print(f\"Encountered an error while processing the following pair:\\nGenerated: {row['Generated Text']}\\nActual: {row['Actual Text']}\")\n","        print(str(e))\n","\n","# Calculate average ROUGE scores\n","average_scores = {\n","    'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n","    'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n","    'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}\n","}\n","\n","num_scores = len(scores)\n","\n","for score in scores:\n","    for rouge_type in average_scores.keys():\n","        average_scores[rouge_type]['f'] += score[rouge_type]['f']\n","        average_scores[rouge_type]['p'] += score[rouge_type]['p']\n","        average_scores[rouge_type]['r'] += score[rouge_type]['r']\n","\n","for rouge_type in average_scores.keys():\n","    average_scores[rouge_type]['f'] /= num_scores\n","    average_scores[rouge_type]['p'] /= num_scores\n","    average_scores[rouge_type]['r'] /= num_scores\n","\n","print(average_scores)\n"]},{"cell_type":"markdown","metadata":{},"source":["For example, results reported in the T5 paper showed that a T5 model fine-tuned on the CNN/Daily Mail dataset achieved a ROUGE-2 F1 score of 16.43. However, these scores are specific to that task and dataset."]},{"cell_type":"markdown","metadata":{},"source":["# BERTScore"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: bert_score in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.3.13)\n","Requirement already satisfied: torch>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert_score) (1.11.0)\n","Requirement already satisfied: pandas>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert_score) (1.5.3)\n","Requirement already satisfied: transformers>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert_score) (4.27.4)\n","Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert_score) (1.23.1)\n","Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert_score) (2.28.2)\n","Requirement already satisfied: tqdm>=4.31.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert_score) (4.65.0)\n","Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert_score) (3.7.1)\n","Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert_score) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2022.7.1)\n","Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.5.0)\n","Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (3.10.7)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.14.1)\n","Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.3.23)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.13.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bert_score) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bert_score) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bert_score) (4.39.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.4)\n","Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bert_score) (9.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->bert_score) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->bert_score) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->bert_score) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->bert_score) (2022.12.7)\n","Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=3.0.0->bert_score) (2023.3.0)\n","Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install bert_score\n","\n","from bert_score import score\n","\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"044e8569adb1463eaa5ca2769cd4b5fb","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e85f3cf491241d7914f715ed1e393a0","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e2cc28987df48edbef30a2e946e0fec","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a31b079c3b4471880c087873e317239","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["calculating scores...\n","computing bert embedding.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c527fa2dffc4a0b9503e57122ba6552","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["computing greedy matching.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75690d7aa1fe4ee3983e0183f623b6e9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["done in 3.30 seconds, 116.52 sentences/sec\n","Precision: 0.8509389758110046\n","Recall: 0.8537600636482239\n","F1 Score: 0.8520768284797668\n"]}],"source":["# Assuming df is your DataFrame with 'Actual Text' and 'Generated Text' columns\n","# df = pd.DataFrame(...)\n","\n","cands = df_output['Generated Text'].tolist() # Candidate (generated) sentences\n","refs = df_output['Actual Text'].tolist() # Reference (actual) sentences\n","\n","P, R, F1 = score(cands, refs, lang='en', verbose=True)\n","\n","print(f\"Precision: {P.mean()}\")\n","print(f\"Recall: {R.mean()}\")\n","print(f\"F1 Score: {F1.mean()}\")"]},{"cell_type":"markdown","metadata":{},"source":["The output you're seeing is a result of the BERTScore, a metric to evaluate the similarity between two pieces of text. In this case, it's comparing your generated text to the actual text. BERTScore is calculated using BERT embeddings, which capture the semantic meaning of words and sentences, and it provides three measurements: precision, recall, and F1 score. Here's what each of them means:\n","\n","Precision (P): Precision is the measure of the number of correctly predicted positive observations out of the total predicted positives. High precision indicates a lower false-positive rate. In your case, a precision score of 0.8509 indicates that about 85.09% of the generated summaries are similar to the actual summaries according to the BERT model.\n","\n","Recall (R): Recall is the measure of the number of correctly predicted positive observations out of the total actual positives. In your case, a recall score of 0.8537 indicates that about 85.37% of the actual summaries are similar to the generated summaries according to the BERT model.\n","\n","F1 Score (F1): The F1 Score is the harmonic mean of precision and recall. It's a balanced measure for scenarios where both precision and recall are important. An F1 score is considered perfect at 1, while the score is worst at 0. In your case, an F1 score of 0.8521 indicates a reasonably good match between the generated summaries and the actual summaries according to the BERT model.\n","\n","Remember, these scores are based on the semantic similarity as captured by the BERT model, so they reflect not just the overlap in terms, but also the similarity in meaning between the generated summaries and the actual summaries."]},{"cell_type":"markdown","metadata":{},"source":["# T5 large\n","\n","see if we can use this model"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask,\n","                #max_length=150,\n","                max_length=50,\n","                num_beams=2,\n","                #repetition_penalty=2.5,\n","                repetition_penalty=1.5,\n","                #length_penalty=1.0,\n","                length_penalty=1.5, # favors longer summaries\n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:7pq0b97o) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b02fbf08e4c41bea5016af5b4104147","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>█▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>5.01627</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">revived-wood-1</strong> at: <a href='https://wandb.ai/team-taj/transformers_t5_large_summarization_capstone/runs/7pq0b97o' target=\"_blank\">https://wandb.ai/team-taj/transformers_t5_large_summarization_capstone/runs/7pq0b97o</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230723_214437-7pq0b97o/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:7pq0b97o). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"589af3c24eb940c68b3f2c07a57da149","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016751007633380745, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/pepperhome/Library/Mobile Documents/com~apple~CloudDocs/My_Desk/Uni/MyCode/MyCapstone/Code/wandb/run-20230723_214712-5f3m1yok</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/team-taj/transformers_t5_large_summarization_capstone/runs/5f3m1yok' target=\"_blank\">dutiful-field-2</a></strong> to <a href='https://wandb.ai/team-taj/transformers_t5_large_summarization_capstone' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/team-taj/transformers_t5_large_summarization_capstone' target=\"_blank\">https://wandb.ai/team-taj/transformers_t5_large_summarization_capstone</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/team-taj/transformers_t5_large_summarization_capstone/runs/5f3m1yok' target=\"_blank\">https://wandb.ai/team-taj/transformers_t5_large_summarization_capstone/runs/5f3m1yok</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["                                                                    text  \\\n","0  Problem caused by your funds being low, Overdrafts and overdraft fees   \n","1  Managing an account, Problem making or receiving payments               \n","2  Trouble during payment process                                          \n","3  Managing an account, Deposits and withdrawals                           \n","4  Struggling to pay mortgage                                              \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ctext  \n","0  summarize: I stopped using my wells Fargo account because Im XXXX they started to take out money even though I didnt have any transactions linked to that account I switched to XXXX XXXX XXXX all my bills moved to that one but money was still being taken out an charged to the wells Fargo account all the way up to now I just recently closed the account but it was negative {$860.00} an my savings was negative {$64.00} because of the over drafts which were scam transactions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n","1  summarize: -On XX/XX/XXXX, I tried to deposit a check in the amount of {$8900.00} through Wells Fargo XXXX XXXX  and received and error message stating that I was over the {$5000.00} daily mobile deposit limit. \\n\\nSo I went to Well Fargo Branch # XXXX XXXX to deposit the check along with requesting {$250.00} cash back from transaction. The teller gave me my cash and advised that there would be a temporary hold on the check but I will have access to all of the funds XX/XX/XXXX. \\n\\nI received a printed receipt showing that {$8700.00} would be delayed XXXX business days and that the date funds would be available is XX/XX/XXXX. \\n\\nXXXX XX/XX/XXXX I received an alert through Wells Fargo XXXX XXXX XXXX XXXX Deposit Hold Alert Deposit Date XX/XX/XXXX Deposit Amount : {$8700.00} Amount Delayed : {$8700.00} Hold Reason : Unusual Deposit Account Activity Date Funds Will Be Available : XX/XX/XXXX -As of XX/XX/XXXX, I still do not have access to my funds. I contacted Wells Fargo and was advised by ( XXXX ) supervisors that XXXX  XXXX XXXX was holding up funds. \\nThen a second supervisor stated that the issue was with the company that issued payment. Which was flat out lies. \\n\\nXXXX  XXXX XXXX and the party that issued payment confirmed confirmation that the check cleared. They also provided the BOA Sequence # XXXX along with WF transaction/sequence # as XXXX. \\n\\nThis has caused a domino effect my transactions and use of my checking account in an attempt to cause me to incur overdraft fees. Wells Fargo is being deceptive in their business practices and illegally withholding my funds.  \n","2  summarize: This is to inform this office that Wells Fargo has lied to this office in its response. \\n\\nI am referencing the response that states \" ... we have been unable to contact XXXX XXXX ''. \\n\\nThat is a FRAUD. I have called on seven occasions. I have left messages. I have emailed. I was told by \" XXXX '' who left a message I would have an answer by XX/XX/XXXX. I was told five times I would receive a call in 24 hours. That was weeks ago. \\n\\nThis is abuse upon abuse. \\n\\nMy equity was STOLEN my life destroyed. My health destroyed. \\n\\nI am demanding the CFPB enforce the order and force repayment of all damages and further punish them for LYING To this body. \\n\\nHOW CAN YOU ALLOW WELLS FARGO TO FURTHER ANUSE AND LIE AND STEAL?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n","3  summarize: I tried to deposit a tax refund check from the internal revenue service on XX/XX/XXXX at the ATM at my local Wells Fargo branch in XXXX XXXX, VA. The refund was long overdue and was in the name of the trust established by my now deceased mother and father, the XXXX Trust. The Trust had an account established at that Wells Fargo for more than a year. The deposit was in the amount of {$22000.00} so it was already difficult that it had taken the IRS years to process and provide the refund check ( long enough that my mother passed away in the interim ). \\n\\nWhen I deposited the check, I was presented with a message telling me that Wells Fargo would not deposit the check and would not return the check to me. They may as well have taken it from my hands and run away. After calling their customer service number and waiting on hold ( and being late for work ), they told me that all I could do was file a complaint and wait 10 days. \\n\\nApparently, I need to wait that long to find out whether the bank will steal my deceased mother 's money or not. I can't believe this is allowed to happen. The check was from the U.S. Internal Revenue Service- not a shady, offshore organization. I don't understand how they can keep that large sum of money with no justification or explanation. It is a lot of money to need to worry may not be returned. If an individual did this, the authorities would take them into custody.                                                                                                                                                                                \n","4  summarize: I believe Wells Fargo fraudulent created and implemented a false Forbearance agreement on my mortgage then sold the mortgage to a servicing company.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n","FULL Dataset: (1918, 2)\n","TRAIN Dataset: (1534, 2)\n","TEST Dataset: (384, 2)\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Initiating Fine-Tuning for the model on our dataset\n"]},{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss:  11.269132614135742\n","Epoch: 0, Loss:  0.21682488918304443\n","Epoch: 1, Loss:  0.216889426112175\n","Epoch: 1, Loss:  0.16202887892723083\n","Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n","Completed 0\n","Completed 100\n","Output Files generated for review\n"]}],"source":["def main():\n","    # WandB – Initialize a new run\n","    wandb.init(project=\"transformers_t5_large_summarization_capstone\")\n","\n","    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n","    # Defining some key variables that will be used later on in the training\n","    config = wandb.config          # Initialize config\n","    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n","    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n","    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n","    config.VAL_EPOCHS = 1\n","    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","    config.SEED = 42               # random seed (default: 42)\n","    config.MAX_LEN = 512\n","    # config.SUMMARY_LEN = 150\n","    config.SUMMARY_LEN = 50\n","\n","    # Set random seeds and deterministic pytorch for reproducibility\n","    torch.manual_seed(config.SEED) # pytorch random seed\n","    np.random.seed(config.SEED) # numpy random seed\n","    torch.backends.cudnn.deterministic = True\n","\n","    # tokenzier for encoding the text\n","    tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n","\n","\n","    # Importing and Pre-Processing the domain data\n","    # Selecting the needed columns only.\n","    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task.\n","    #df = pd.read_csv('./data/news_summary.csv',encoding='latin-1')\n","    df = pd.read_csv('data/complaints1_prepped.csv',encoding='latin-1')\n","\n","    df = df[['text','ctext']]\n","    df.ctext = 'summarize: ' + df.ctext\n","    print(df.head())\n","\n","\n","    # Creation of Dataset and Dataloader\n","    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation.\n","    train_size = 0.8\n","    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n","    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","    train_dataset = train_dataset.reset_index(drop=True)\n","\n","    print(\"FULL Dataset: {}\".format(df.shape))\n","    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n","\n","\n","    # Creating the Training and Validation dataset for further creation of Dataloader\n","    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","\n","    # Defining the parameters for creation of dataloaders\n","    train_params = {\n","        'batch_size': config.TRAIN_BATCH_SIZE,\n","        'shuffle': True,\n","        'num_workers': 0\n","        }\n","\n","    val_params = {\n","        'batch_size': config.VALID_BATCH_SIZE,\n","        'shuffle': False,\n","        'num_workers': 0\n","        }\n","\n","    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","    training_loader = DataLoader(training_set, **train_params)\n","    val_loader = DataLoader(val_set, **val_params)\n","\n","\n","\n","    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n","    # Further this model is sent to device (GPU/TPU) for using the hardware.\n","    model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n","    model = model.to(device)\n","\n","    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n","    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n","\n","    # Log metrics with wandb\n","    wandb.watch(model, log=\"all\")\n","    # Training loop\n","    print('Initiating Fine-Tuning for the model on our dataset')\n","\n","    for epoch in range(config.TRAIN_EPOCHS):\n","        train(epoch, tokenizer, model, device, training_loader, optimizer)\n","\n","\n","    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n","    # Saving the dataframe as predictions.csv\n","    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","    for epoch in range(config.VAL_EPOCHS):\n","        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","        #final_df.to_csv('./models/predictions.csv')\n","        final_df.to_csv('models/complaints2_predictions_t5large.csv')\n","        print('Output Files generated for review')\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Generated Text</th>\n","      <th>Actual Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Deposits and withdrawals</td>\n","      <td>Managing an account, Problem making or receiving payments</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0            Generated Text  \\\n","0  0           Deposits and withdrawals   \n","\n","                                                 Actual Text  \n","0  Managing an account, Problem making or receiving payments  "]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["df_output_t5large = pd.read_csv('models/complaints2_predictions_t5large.csv',encoding='latin-1')\n","df_output_t5large.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# BertScore T5-large"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["calculating scores...\n","computing bert embedding.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2f1322a393444b8a7c7c667f5949db3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["computing greedy matching.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12d78b7ccba54e49a5045b410c7d992f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["done in 3.60 seconds, 106.53 sentences/sec\n","Precision: 0.8503473401069641\n","Recall: 0.8551210761070251\n","F1 Score: 0.8524850010871887\n"]}],"source":["# Assuming df is your DataFrame with 'Actual Text' and 'Generated Text' columns\n","# df = pd.DataFrame(...)\n","from bert_score import score\n","cands = df_output_t5large['Generated Text'].tolist() # Candidate (generated) sentences\n","refs = df_output_t5large['Actual Text'].tolist() # Reference (actual) sentences\n","\n","P, R, F1 = score(cands, refs, lang='en', verbose=True)\n","\n","print(f\"Precision: {P.mean()}\")\n","print(f\"Recall: {R.mean()}\")\n","print(f\"F1 Score: {F1.mean()}\")"]},{"cell_type":"markdown","metadata":{},"source":["# T5 3b"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask,\n","                #max_length=150,\n","                max_length=50,\n","                num_beams=2,\n","                #repetition_penalty=2.5,\n","                repetition_penalty=1.5,\n","                #length_penalty=1.0,\n","                length_penalty=1.5, # favors longer summaries\n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:f9xgphng) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24856bf790164f238f642bfc6c3e4fea","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>6.56872</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">iconic-blaze-1</strong> at: <a href='https://wandb.ai/team-taj/transformers_t5_3b_summarization_capstone/runs/f9xgphng' target=\"_blank\">https://wandb.ai/team-taj/transformers_t5_3b_summarization_capstone/runs/f9xgphng</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230724_084930-f9xgphng/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:f9xgphng). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab06750e77974c3d9c7018b06671eff8","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01683507291672868, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/pepperhome/Library/Mobile Documents/com~apple~CloudDocs/My_Desk/Uni/MyCode/MyCapstone/Code/wandb/run-20230724_085054-cpq9xctn</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/team-taj/transformers_t5_3b_summarization_capstone/runs/cpq9xctn' target=\"_blank\">unique-sound-2</a></strong> to <a href='https://wandb.ai/team-taj/transformers_t5_3b_summarization_capstone' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/team-taj/transformers_t5_3b_summarization_capstone' target=\"_blank\">https://wandb.ai/team-taj/transformers_t5_3b_summarization_capstone</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/team-taj/transformers_t5_3b_summarization_capstone/runs/cpq9xctn' target=\"_blank\">https://wandb.ai/team-taj/transformers_t5_3b_summarization_capstone/runs/cpq9xctn</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["                                                                    text  \\\n","0  Problem caused by your funds being low, Overdrafts and overdraft fees   \n","1  Managing an account, Problem making or receiving payments               \n","2  Trouble during payment process                                          \n","3  Managing an account, Deposits and withdrawals                           \n","4  Struggling to pay mortgage                                              \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ctext  \n","0  summarize: I stopped using my wells Fargo account because Im XXXX they started to take out money even though I didnt have any transactions linked to that account I switched to XXXX XXXX XXXX all my bills moved to that one but money was still being taken out an charged to the wells Fargo account all the way up to now I just recently closed the account but it was negative {$860.00} an my savings was negative {$64.00} because of the over drafts which were scam transactions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n","1  summarize: -On XX/XX/XXXX, I tried to deposit a check in the amount of {$8900.00} through Wells Fargo XXXX XXXX  and received and error message stating that I was over the {$5000.00} daily mobile deposit limit. \\n\\nSo I went to Well Fargo Branch # XXXX XXXX to deposit the check along with requesting {$250.00} cash back from transaction. The teller gave me my cash and advised that there would be a temporary hold on the check but I will have access to all of the funds XX/XX/XXXX. \\n\\nI received a printed receipt showing that {$8700.00} would be delayed XXXX business days and that the date funds would be available is XX/XX/XXXX. \\n\\nXXXX XX/XX/XXXX I received an alert through Wells Fargo XXXX XXXX XXXX XXXX Deposit Hold Alert Deposit Date XX/XX/XXXX Deposit Amount : {$8700.00} Amount Delayed : {$8700.00} Hold Reason : Unusual Deposit Account Activity Date Funds Will Be Available : XX/XX/XXXX -As of XX/XX/XXXX, I still do not have access to my funds. I contacted Wells Fargo and was advised by ( XXXX ) supervisors that XXXX  XXXX XXXX was holding up funds. \\nThen a second supervisor stated that the issue was with the company that issued payment. Which was flat out lies. \\n\\nXXXX  XXXX XXXX and the party that issued payment confirmed confirmation that the check cleared. They also provided the BOA Sequence # XXXX along with WF transaction/sequence # as XXXX. \\n\\nThis has caused a domino effect my transactions and use of my checking account in an attempt to cause me to incur overdraft fees. Wells Fargo is being deceptive in their business practices and illegally withholding my funds.  \n","2  summarize: This is to inform this office that Wells Fargo has lied to this office in its response. \\n\\nI am referencing the response that states \" ... we have been unable to contact XXXX XXXX ''. \\n\\nThat is a FRAUD. I have called on seven occasions. I have left messages. I have emailed. I was told by \" XXXX '' who left a message I would have an answer by XX/XX/XXXX. I was told five times I would receive a call in 24 hours. That was weeks ago. \\n\\nThis is abuse upon abuse. \\n\\nMy equity was STOLEN my life destroyed. My health destroyed. \\n\\nI am demanding the CFPB enforce the order and force repayment of all damages and further punish them for LYING To this body. \\n\\nHOW CAN YOU ALLOW WELLS FARGO TO FURTHER ANUSE AND LIE AND STEAL?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n","3  summarize: I tried to deposit a tax refund check from the internal revenue service on XX/XX/XXXX at the ATM at my local Wells Fargo branch in XXXX XXXX, VA. The refund was long overdue and was in the name of the trust established by my now deceased mother and father, the XXXX Trust. The Trust had an account established at that Wells Fargo for more than a year. The deposit was in the amount of {$22000.00} so it was already difficult that it had taken the IRS years to process and provide the refund check ( long enough that my mother passed away in the interim ). \\n\\nWhen I deposited the check, I was presented with a message telling me that Wells Fargo would not deposit the check and would not return the check to me. They may as well have taken it from my hands and run away. After calling their customer service number and waiting on hold ( and being late for work ), they told me that all I could do was file a complaint and wait 10 days. \\n\\nApparently, I need to wait that long to find out whether the bank will steal my deceased mother 's money or not. I can't believe this is allowed to happen. The check was from the U.S. Internal Revenue Service- not a shady, offshore organization. I don't understand how they can keep that large sum of money with no justification or explanation. It is a lot of money to need to worry may not be returned. If an individual did this, the authorities would take them into custody.                                                                                                                                                                                \n","4  summarize: I believe Wells Fargo fraudulent created and implemented a false Forbearance agreement on my mortgage then sold the mortgage to a servicing company.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n","FULL Dataset: (1918, 2)\n","TRAIN Dataset: (1534, 2)\n","TEST Dataset: (384, 2)\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Initiating Fine-Tuning for the model on our dataset\n"]},{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss:  11.667095184326172\n","Epoch: 0, Loss:  0.5319582223892212\n","Epoch: 1, Loss:  0.325722336769104\n","Epoch: 1, Loss:  0.3637794554233551\n","Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n","Completed 0\n","Completed 100\n","Output Files generated for review\n"]}],"source":["def main():\n","    # WandB – Initialize a new run\n","    wandb.init(project=\"transformers_t5_3b_summarization_capstone\")\n","\n","    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n","    # Defining some key variables that will be used later on in the training\n","    config = wandb.config          # Initialize config\n","    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n","    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n","    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n","    config.VAL_EPOCHS = 1\n","    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","    config.SEED = 44               # random seed (default: 42)\n","    config.MAX_LEN = 512\n","    # config.SUMMARY_LEN = 150\n","    config.SUMMARY_LEN = 50\n","\n","    # Set random seeds and deterministic pytorch for reproducibility\n","    torch.manual_seed(config.SEED) # pytorch random seed\n","    np.random.seed(config.SEED) # numpy random seed\n","    torch.backends.cudnn.deterministic = True\n","\n","    # tokenzier for encoding the text\n","    tokenizer = T5Tokenizer.from_pretrained(\"t5-3b\")\n","\n","\n","    # Importing and Pre-Processing the domain data\n","    # Selecting the needed columns only.\n","    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task.\n","    #df = pd.read_csv('./data/news_summary.csv',encoding='latin-1')\n","    df = pd.read_csv('data/complaints1_prepped.csv',encoding='latin-1')\n","\n","    df = df[['text','ctext']]\n","    df.ctext = 'summarize: ' + df.ctext\n","    print(df.head())\n","\n","\n","    # Creation of Dataset and Dataloader\n","    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation.\n","    train_size = 0.8\n","    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n","    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","    train_dataset = train_dataset.reset_index(drop=True)\n","\n","    print(\"FULL Dataset: {}\".format(df.shape))\n","    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n","\n","\n","    # Creating the Training and Validation dataset for further creation of Dataloader\n","    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","\n","    # Defining the parameters for creation of dataloaders\n","    train_params = {\n","        'batch_size': config.TRAIN_BATCH_SIZE,\n","        'shuffle': True,\n","        'num_workers': 0\n","        }\n","\n","    val_params = {\n","        'batch_size': config.VALID_BATCH_SIZE,\n","        'shuffle': False,\n","        'num_workers': 0\n","        }\n","\n","    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","    training_loader = DataLoader(training_set, **train_params)\n","    val_loader = DataLoader(val_set, **val_params)\n","\n","\n","\n","    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n","    # Further this model is sent to device (GPU/TPU) for using the hardware.\n","    model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")\n","    model = model.to(device)\n","\n","    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n","    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n","\n","    # Log metrics with wandb\n","    wandb.watch(model, log=\"all\")\n","    # Training loop\n","    print('Initiating Fine-Tuning for the model on our dataset')\n","\n","    for epoch in range(config.TRAIN_EPOCHS):\n","        train(epoch, tokenizer, model, device, training_loader, optimizer)\n","\n","\n","    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n","    # Saving the dataframe as predictions.csv\n","    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","    for epoch in range(config.VAL_EPOCHS):\n","        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","        #final_df.to_csv('./models/predictions.csv')\n","        final_df.to_csv('models/complaints3_predictions_t5 3b.csv')\n","        print('Output Files generated for review')\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Generated Text</th>\n","      <th>Actual Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</td>\n","      <td>Managing the loan or lease, Billing problem</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                    Generated Text  \\\n","0  0           XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX   \n","\n","                                   Actual Text  \n","0  Managing the loan or lease, Billing problem  "]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["df_output_t53b = pd.read_csv('models/complaints3_predictions_t5 3b.csv',encoding='latin-1')\n","df_output_t53b.head(1) \n","\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["calculating scores...\n","computing bert embedding.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e0b36fe55614488b79886a445432b38","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["computing greedy matching.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a7f050991c847848f57890f277b6003","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["done in 2.88 seconds, 133.25 sentences/sec\n","Precision: 0.8100943565368652\n","Recall: 0.8408690094947815\n","F1 Score: 0.8241753578186035\n"]}],"source":["\n","from bert_score import score\n","cands = df_output_t53b['Generated Text'].tolist() # Candidate (generated) sentences\n","refs = df_output_t53b['Actual Text'].tolist() # Reference (actual) sentences\n","\n","P, R, F1 = score(cands, refs, lang='en', verbose=True)\n","\n","print(f\"Precision: {P.mean()}\")\n","print(f\"Recall: {R.mean()}\")\n","print(f\"F1 Score: {F1.mean()}\")"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["calculating scores...\n","computing bert embedding.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1d33a4a29674afcacdb8e6ac730c1f7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["computing greedy matching.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e5c16acdac443a686228d9d813e8574","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["done in 2.54 seconds, 112.03 sentences/sec\n","Precision: 0.8550873398780823\n","Recall: 0.8565149903297424\n","F1 Score: 0.8556248545646667\n"]}],"source":["df_output_t53b_v2 = pd.read_csv('models/complaints3_predictions_t5 3b_v2.csv',encoding='latin-1')\n","df_output_t53b_v2.head(1) \n","\n","from bert_score import score\n","cands = df_output_t53b_v2['Generated Text'].tolist() # Candidate (generated) sentences\n","refs = df_output_t53b_v2['Actual Text'].tolist() # Reference (actual) sentences\n","\n","P, R, F1 = score(cands, refs, lang='en', verbose=True)\n","\n","print(f\"Precision: {P.mean()}\")\n","print(f\"Recall: {R.mean()}\")\n","print(f\"F1 Score: {F1.mean()}\")"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["calculating scores...\n","computing bert embedding.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d96a1688f82411e92404140a767d3c9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["computing greedy matching.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"16d255daded74f20817b10504a0e6fe2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["done in 2.20 seconds, 119.91 sentences/sec\n","Precision: 0.8560411334037781\n","Recall: 0.8568480014801025\n","F1 Score: 0.8562694787979126\n"]}],"source":["df_output_t53b_v3 = pd.read_csv('models/complaints3_predictions_t5 3b_v3.csv',encoding='latin-1')\n","df_output_t53b_v3.head(1) \n","\n","from bert_score import score\n","cands = df_output_t53b_v3['Generated Text'].tolist() # Candidate (generated) sentences\n","refs = df_output_t53b_v3['Actual Text'].tolist() # Reference (actual) sentences\n","\n","P, R, F1 = score(cands, refs, lang='en', verbose=True)\n","\n","print(f\"Precision: {P.mean()}\")\n","print(f\"Recall: {R.mean()}\")\n","print(f\"F1 Score: {F1.mean()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Using the bigger data set\n","## Using the t5-large\n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["read_df = pd.read_csv('data/complaints2.csv',encoding='latin-1')\n","read_df['text'] = read_df.apply(concat_fields, axis=1)\n","read_df = read_df.rename(columns={'Consumer complaint narrative': 'ctext'})\n","\n","\n","df = read_df[['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue', 'text', 'ctext']]\n","\n","\n","df.to_csv('data/complaints2_prepped.csv', index=False)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/k9/w33jch99031byvrz4c83qqgm0000gn/T/ipykernel_43449/290252226.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['text_length'] = df['text'].apply(lambda x: len(str(x)))\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame and 'ctext' is your text column\n","# df = pd.DataFrame(...)\n","\n","# Calculate the length of each text in the 'ctext' column\n","df['text_length'] = df['text'].apply(lambda x: len(str(x)))\n","\n","# Plot the distribution of text lengths\n","plt.hist(df['text_length'], bins='auto') # 'auto' allows matplotlib to choose the number of bins optimally\n","plt.title('Distribution of Summary Text Lengths')\n","plt.xlabel('Summary Text Length')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/k9/w33jch99031byvrz4c83qqgm0000gn/T/ipykernel_43449/1199738667.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['num_tokens'] = df['text'].apply(lambda x: len(str(x).split()))\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame and 'ctext' is your text column\n","# df = pd.DataFrame(...)\n","\n","# Calculate the number of tokens in each text in the 'ctext' column\n","df['num_tokens'] = df['text'].apply(lambda x: len(str(x).split()))\n","\n","# Plot the distribution of text lengths\n","plt.hist(df['num_tokens'], bins='auto') # 'auto' allows matplotlib to choose the number of bins optimally\n","plt.title('Distribution of Summary Text Tokens')\n","plt.xlabel('Summary Text Tokens')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:gnns8lfa) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"]},{"data":{"text/html":["Successfully finished last run (ID:gnns8lfa). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9bda3fb96cb347ba853ad73b16e58b30","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01675081736660407, max=1.0)…"]},"metadata":{},"output_type":"display_data"}],"source":["def main():\n","    # WandB – Initialize a new run\n","    wandb.init(project=\"transformers_summarization_capstone_t5large_full_dataset\")\n","\n","    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n","    # Defining some key variables that will be used later on in the training\n","    config = wandb.config          # Initialize config\n","    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n","    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n","    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n","    config.VAL_EPOCHS = 1\n","    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","    config.SEED = 42               # random seed (default: 42)\n","    config.MAX_LEN = 512\n","    # config.SUMMARY_LEN = 150\n","    config.SUMMARY_LEN = 30\n","\n","    # Set random seeds and deterministic pytorch for reproducibility\n","    torch.manual_seed(config.SEED) # pytorch random seed\n","    np.random.seed(config.SEED) # numpy random seed\n","    torch.backends.cudnn.deterministic = True\n","\n","    # tokenzier for encoding the text\n","    tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n","\n","\n","    # Importing and Pre-Processing the domain data\n","    # Selecting the needed columns only.\n","    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task.\n","    #df = pd.read_csv('./data/news_summary.csv',encoding='latin-1')\n","    df = pd.read_csv('data/complaints2_prepped.csv',encoding='latin-1')\n","\n","    df = df[['text','ctext']]\n","    df.ctext = 'summarize: ' + df.ctext\n","    print(df.head())\n","\n","\n","    # Creation of Dataset and Dataloader\n","    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation.\n","    train_size = 0.8\n","    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n","    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","    train_dataset = train_dataset.reset_index(drop=True)\n","\n","    print(\"FULL Dataset: {}\".format(df.shape))\n","    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n","\n","\n","    # Creating the Training and Validation dataset for further creation of Dataloader\n","    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","\n","    # Defining the parameters for creation of dataloaders\n","    train_params = {\n","        'batch_size': config.TRAIN_BATCH_SIZE,\n","        'shuffle': True,\n","        'num_workers': 0\n","        }\n","\n","    val_params = {\n","        'batch_size': config.VALID_BATCH_SIZE,\n","        'shuffle': False,\n","        'num_workers': 0\n","        }\n","\n","    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","    training_loader = DataLoader(training_set, **train_params)\n","    val_loader = DataLoader(val_set, **val_params)\n","\n","\n","\n","    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n","    # Further this model is sent to device (GPU/TPU) for using the hardware.\n","    model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n","    model = model.to(device)\n","\n","    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n","    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n","\n","    # Log metrics with wandb\n","    wandb.watch(model, log=\"all\")\n","    # Training loop\n","    print('Initiating Fine-Tuning for the model on our dataset')\n","\n","    for epoch in range(config.TRAIN_EPOCHS):\n","        train(epoch, tokenizer, model, device, training_loader, optimizer)\n","\n","\n","    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n","    # Saving the dataframe as predictions.csv\n","    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","    for epoch in range(config.VAL_EPOCHS):\n","        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","        #final_df.to_csv('./models/predictions.csv')\n","        final_df.to_csv('models/complaints2_predictions_t5large_full_dataset.csv')\n","        print('Output Files generated for review')\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_output = pd.read_csv('models/complaints2_predictions_t5large_full_dataset.csv',encoding='latin-1')\n","\n","# Assuming df is your DataFrame with 'Actual Text' and 'Generated Text' columns\n","\n","from bert_score import score\n","cands = df_output['Generated Text'].tolist() # Candidate (generated) sentences\n","refs = df_output['Actual Text'].tolist() # Reference (actual) sentences\n","\n","P, R, F1 = score(cands, refs, lang='en', verbose=True)\n","\n","print(f\"Precision: {P.mean()}\")\n","print(f\"Recall: {R.mean()}\")\n","print(f\"F1 Score: {F1.mean()}\")"]},{"cell_type":"markdown","metadata":{},"source":["# T5-3b\n","## Full dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def main():\n","    # WandB – Initialize a new run\n","    wandb.init(project=\"transformers_summarization_capstone_t5-3b_full_dataset\")\n","\n","    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n","    # Defining some key variables that will be used later on in the training\n","    config = wandb.config          # Initialize config\n","    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n","    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n","    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n","    config.VAL_EPOCHS = 1\n","    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","    config.SEED = 42               # random seed (default: 42)\n","    config.MAX_LEN = 512\n","    # config.SUMMARY_LEN = 150\n","    config.SUMMARY_LEN = 30\n","\n","    # Set random seeds and deterministic pytorch for reproducibility\n","    torch.manual_seed(config.SEED) # pytorch random seed\n","    np.random.seed(config.SEED) # numpy random seed\n","    torch.backends.cudnn.deterministic = True\n","\n","    # tokenzier for encoding the text\n","    tokenizer = T5Tokenizer.from_pretrained(\"t5-3b\")\n","\n","\n","    # Importing and Pre-Processing the domain data\n","    # Selecting the needed columns only.\n","    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task.\n","    #df = pd.read_csv('./data/news_summary.csv',encoding='latin-1')\n","    df = pd.read_csv('data/complaints2_prepped.csv',encoding='latin-1')\n","\n","    df = df[['text','ctext']]\n","    df.ctext = 'summarize: ' + df.ctext\n","    print(df.head())\n","\n","\n","    # Creation of Dataset and Dataloader\n","    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation.\n","    train_size = 0.8\n","    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n","    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","    train_dataset = train_dataset.reset_index(drop=True)\n","\n","    print(\"FULL Dataset: {}\".format(df.shape))\n","    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n","\n","\n","    # Creating the Training and Validation dataset for further creation of Dataloader\n","    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","\n","    # Defining the parameters for creation of dataloaders\n","    train_params = {\n","        'batch_size': config.TRAIN_BATCH_SIZE,\n","        'shuffle': True,\n","        'num_workers': 0\n","        }\n","\n","    val_params = {\n","        'batch_size': config.VALID_BATCH_SIZE,\n","        'shuffle': False,\n","        'num_workers': 0\n","        }\n","\n","    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","    training_loader = DataLoader(training_set, **train_params)\n","    val_loader = DataLoader(val_set, **val_params)\n","\n","\n","\n","    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n","    # Further this model is sent to device (GPU/TPU) for using the hardware.\n","    model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")\n","    model = model.to(device)\n","\n","    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n","    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n","\n","    # Log metrics with wandb\n","    wandb.watch(model, log=\"all\")\n","    # Training loop\n","    print('Initiating Fine-Tuning for the model on our dataset')\n","\n","    for epoch in range(config.TRAIN_EPOCHS):\n","        train(epoch, tokenizer, model, device, training_loader, optimizer)\n","\n","\n","    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n","    # Saving the dataframe as predictions.csv\n","    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","    for epoch in range(config.VAL_EPOCHS):\n","        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","        #final_df.to_csv('./models/predictions.csv')\n","        final_df.to_csv('models/complaints2_predictions_t5 3b_full_dataset.csv')\n","        print('Output Files generated for review')\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_output = pd.read_csv('models/complaints2_predictions_t5 3b_full_dataset.csv',encoding='latin-1')\n","\n","# Assuming df is your DataFrame with 'Actual Text' and 'Generated Text' columns\n","\n","from bert_score import score\n","cands = df_output['Generated Text'].tolist() # Candidate (generated) sentences\n","refs = df_output['Actual Text'].tolist() # Reference (actual) sentences\n","\n","P, R, F1 = score(cands, refs, lang='en', verbose=True)\n","\n","print(f\"Precision: {P.mean()}\")\n","print(f\"Recall: {R.mean()}\")\n","print(f\"F1 Score: {F1.mean()}\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb","timestamp":1690038624764}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"0db34048ca4c4563ab073cf78d4d04a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_6beb76bc3adf49cb90eeb145d2e9b85a","IPY_MODEL_f04b3e727b8c4869b844e429cbc9ec47"],"layout":"IPY_MODEL_35dcf351426a4a569783c64e2c2afaab"}},"20326ba4a1ac454e9ef7e5db0d08e0ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35dcf351426a4a569783c64e2c2afaab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43a0d0df745e453383259065bc7d76c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4bd9b972524d403fbc356be68b9d188e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bff151e590184f5aaed5a034919294ea","placeholder":"​","style":"IPY_MODEL_c9edd3ead5714b01864b32fc3a4283d5","value":"0.001 MB of 0.012 MB uploaded (0.000 MB deduped)\r"}},"6b802411f9e5449ba756d696d6e5293c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6beb76bc3adf49cb90eeb145d2e9b85a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b802411f9e5449ba756d696d6e5293c","placeholder":"​","style":"IPY_MODEL_43a0d0df745e453383259065bc7d76c4","value":"Waiting for wandb.init()...\r"}},"6f156748cc37450d9c5312952a86901a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7aa920950cd343b780fb14b89f0e791a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bffde4b3a08b4fecb6ca73027fe800a2","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c446e2d887ac468db6381b5cb905f7fa","value":0.09433369957620467}},"bff151e590184f5aaed5a034919294ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bffde4b3a08b4fecb6ca73027fe800a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c446e2d887ac468db6381b5cb905f7fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c68bbd49e1f746fb84e1ab665abbb169":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9edd3ead5714b01864b32fc3a4283d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1d182d7487342809470665d39359c3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_4bd9b972524d403fbc356be68b9d188e","IPY_MODEL_7aa920950cd343b780fb14b89f0e791a"],"layout":"IPY_MODEL_20326ba4a1ac454e9ef7e5db0d08e0ec"}},"f04b3e727b8c4869b844e429cbc9ec47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c68bbd49e1f746fb84e1ab665abbb169","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f156748cc37450d9c5312952a86901a","value":1}}}}},"nbformat":4,"nbformat_minor":0}
