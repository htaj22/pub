{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Reading: Topic Modeling\n",
    "Note: This page is only intended for more advanced readers with existing python knowledge, and it is not required reading for this class.\n",
    "\n",
    "1. Many software packages, including R, SAS, STATA, and even Excel, now have their own package or modules to handle text related analytic tasks. However, I found that Python is the most powerful and user friendly one.\n",
    "\n",
    "2. There are many existing packages (libraries) in Python that can be used to for run all kinds of text analytics. One of the most popular ones that can handle many common tasks is Gensim. Gensim was primarily developed for topic modeling. However, it now supports a variety of other NLP tasks such as converting words to vectors (word2vec), document to vectors (doc2vec), finding text similarity, and text summarization.\n",
    "\n",
    "3. below is the a brief Intro to Gensim that I copied from the internet for your convenience (existing python knowledge required). However, if you are interested in knowing more, https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ Links to an external site. this page provides an excellent step-by-step guide on doing topic Modeling with Gensim. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Gensim\n",
    "If you use pip installer to install your Python libraries, you can use the following command to download the Gensim library:\n",
    "\n",
    "$ pip install gensim\n",
    "Alternatively, if you use the Anaconda distribution of Python, you can execute the following command to install the Gensim library:\n",
    "\n",
    "$ conda install -c anaconda gensim\n",
    "Let's now see how we can perform different NLP tasks using the Gensim library.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dictionaries\n",
    "Statistical algorithms work with numbers, however, natural languages contain data in the form of text. Therefore, a mechanism is needed to convert words to numbers. Similarly, after applying different types of processes on the numbers, we need to convert numbers back to text.\n",
    "\n",
    "One way to achieve this type of functionality is to create a dictionary that assigns a numeric ID to every unique word in the document. The dictionary can then be used to find the numeric equivalent of a word and vice versa.\n",
    "\n",
    "Creating Dictionaries using In-Memory Objects\n",
    "It is super easy to create dictionaries that map words to IDs using Python's Gensim library. Look at the following script:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has: 46 tokens\n",
      "(AI),                    0\n",
      "AI                       1\n",
      "Computer                 2\n",
      "In                       3\n",
      "achieving                4\n",
      "actions                  5\n",
      "agents:                  6\n",
      "and                      7\n",
      "animals.                 8\n",
      "any                      9\n",
      "artificial              10\n",
      "as                      11\n",
      "by                      12\n",
      "called                  13\n",
      "chance                  14\n",
      "computer                15\n",
      "contrast                16\n",
      "defines                 17\n",
      "demonstrated            18\n",
      "device                  19\n",
      "displayed               20\n",
      "environment             21\n",
      "goals.                  22\n",
      "humans                  23\n",
      "in                      24\n",
      "intelligence            25\n",
      "intelligence,           26\n",
      "intelligent             27\n",
      "is                      28\n",
      "its                     29\n",
      "machine                 30\n",
      "machines,               31\n",
      "maximize                32\n",
      "natural                 33\n",
      "of                      34\n",
      "perceives               35\n",
      "research                36\n",
      "science                 37\n",
      "science,                38\n",
      "sometimes               39\n",
      "study                   40\n",
      "successfully            41\n",
      "takes                   42\n",
      "that                    43\n",
      "the                     44\n",
      "to                      45\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "text = [\"\"\"In computer science, artificial intelligence (AI),\n",
    "             sometimes called machine intelligence, is intelligence\n",
    "             demonstrated by machines, in contrast to the natural intelligence\n",
    "             displayed by humans and animals. Computer science defines\n",
    "             AI research as the study of intelligent agents: any device that\n",
    "             perceives its environment and takes actions that maximize its chance\n",
    "             of successfully achieving its goals.\"\"\"]\n",
    "\n",
    "tokens = [[token for token in sentence.split()] for sentence in text]\n",
    "gensim_dictionary = corpora.Dictionary(tokens)\n",
    "\n",
    "print(\"The dictionary has: \" +str(len(gensim_dictionary)) + \" tokens\")\n",
    "\n",
    "for k, v in gensim_dictionary.token2id.items():\n",
    "    print(f'{k:{15}} {v:{10}}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, we first import the gensim library along with the corpora module from the library. Next, we have some text (which is the first part of the first paragraph of the Wikipedia article on Artificial Intelligence) stored in the text variable.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a dictionary, we need a list of words from our text (also known as tokens). In the following line, we split our document into sentences and then the sentences into words.\n",
    "\n",
    "tokens = [[token for token in sentence.split()] for sentence in text]\n",
    "\n",
    "We are now ready to create our dictionary. To do so, we can use the Dictionary object of the corpora module and pass it the list of tokens.\n",
    "\n",
    "Finally, to print the contents of the newly created dictionary, we can use the token2id object of the Dictionary class. The output of the script above looks like above.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows each unique word in our text along with the numeric ID that the word has been assigned. The word or token is the key of the dictionary and the ID is the value. You can also see the Id assigned to the individual word using the following script:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(gensim_dictionary.token2id[\"study\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, we pass the word \"study\" as the key to our dictionary. In the output, you should see the corresponding value i.e. the ID of the word \"study\", which is 40.\n",
    "\n",
    "Similarly, you can use the following script to find the key or word for a specific ID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim_dictionary.token2id.keys())[list(gensim_dictionary.token2id.values()).index(40)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print the tokens and their corresponding IDs we used a for-loop. However, you can directly print the tokens and their IDs by printing the dictionary, as shown here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(AI),': 0, 'AI': 1, 'Computer': 2, 'In': 3, 'achieving': 4, 'actions': 5, 'agents:': 6, 'and': 7, 'animals.': 8, 'any': 9, 'artificial': 10, 'as': 11, 'by': 12, 'called': 13, 'chance': 14, 'computer': 15, 'contrast': 16, 'defines': 17, 'demonstrated': 18, 'device': 19, 'displayed': 20, 'environment': 21, 'goals.': 22, 'humans': 23, 'in': 24, 'intelligence': 25, 'intelligence,': 26, 'intelligent': 27, 'is': 28, 'its': 29, 'machine': 30, 'machines,': 31, 'maximize': 32, 'natural': 33, 'of': 34, 'perceives': 35, 'research': 36, 'science': 37, 'science,': 38, 'sometimes': 39, 'study': 40, 'successfully': 41, 'takes': 42, 'that': 43, 'the': 44, 'to': 45}\n"
     ]
    }
   ],
   "source": [
    "print(gensim_dictionary.token2id)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might not be as clear as the one printed using the loop, although it still serves its purpose.\n",
    "\n",
    "Let's now see how we can add more tokens to an existing dictionary using a new document. Look at the following script:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has: 65 tokens\n",
      "{'(AI),': 0, 'AI': 1, 'Computer': 2, 'In': 3, 'achieving': 4, 'actions': 5, 'agents:': 6, 'and': 7, 'animals.': 8, 'any': 9, 'artificial': 10, 'as': 11, 'by': 12, 'called': 13, 'chance': 14, 'computer': 15, 'contrast': 16, 'defines': 17, 'demonstrated': 18, 'device': 19, 'displayed': 20, 'environment': 21, 'goals.': 22, 'humans': 23, 'in': 24, 'intelligence': 25, 'intelligence,': 26, 'intelligent': 27, 'is': 28, 'its': 29, 'machine': 30, 'machines,': 31, 'maximize': 32, 'natural': 33, 'of': 34, 'perceives': 35, 'research': 36, 'science': 37, 'science,': 38, 'sometimes': 39, 'study': 40, 'successfully': 41, 'takes': 42, 'that': 43, 'the': 44, 'to': 45, '\"artificial': 46, '\"cognitive\"': 47, '\"learning\"': 48, '\"problem': 49, 'Colloquially,': 50, 'associate': 51, 'describe': 52, 'functions': 53, 'human': 54, 'intelligence\"': 55, 'machines': 56, 'mimic': 57, 'minds,': 58, 'other': 59, 'solving': 60, 'such': 61, 'term': 62, 'used': 63, 'with': 64}\n"
     ]
    }
   ],
   "source": [
    "text = [\"\"\"Colloquially, the term \"artificial intelligence\" is used to\n",
    "           describe machines that mimic \"cognitive\" functions that humans\n",
    "           associate with other human minds, such as \"learning\" and \"problem solving\"\"\"]\n",
    "\n",
    "tokens = [[token for token in sentence.split()] for sentence in text]\n",
    "gensim_dictionary.add_documents(tokens)\n",
    "\n",
    "print(\"The dictionary has: \" + str(len(gensim_dictionary)) + \" tokens\")\n",
    "print(gensim_dictionary.token2id)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above we have a new document that contains the second part of the first paragraph of the Wikipedia article on Artificial Intelligence. We split the text into tokens and then simply call the add_documents method to add the tokens to our existing dictionary. Finally, we print the updated dictionary on the console.\n",
    "\n",
    "You can see that now we have 65 tokens in our dictionary, while previously we had 45 tokens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dictionaries using Text Files\n",
    "\n",
    "In the previous section, we had in-memory text. What if we want to create a dictionary by reading a text file from the hard drive? To do so, we can use the simple_process method from the gensim.utils library. The advantage of using this method is that it reads the text file line by line and returns the tokens in the line. You don't have to load the complete text file in the memory in order to create a dictionary.\n",
    "\n",
    "Before executing the next example, create a file \"file1.txt\" and add the following text to the file (this is the first half of the first paragraph of the Wikipedia article on Global Warming).\n",
    "\n",
    "\n",
    "Global warming is a long-term rise in the average temperature of the Earth's climate system, an aspect of climate change shown by temperature measurements and by multiple effects of the warming. Though earlier geological periods also experienced episodes of warming, the term commonly refers to the observed and continuing increase in average air and ocean temperatures since 1900 caused mainly by emissions of greenhouse gasses in the modern industrial economy.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a dictionary that will contain tokens from the text file \"file1.txt\":\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'air': 0, 'also': 1, 'an': 2, 'and': 3, 'aspect': 4, 'average': 5, 'by': 6, 'caused': 7, 'change': 8, 'climate': 9, 'commonly': 10, 'continuing': 11, 'earlier': 12, 'earth': 13, 'economy': 14, 'effects': 15, 'emissions': 16, 'episodes': 17, 'experienced': 18, 'gasses': 19, 'geological': 20, 'global': 21, 'greenhouse': 22, 'in': 23, 'increase': 24, 'industrial': 25, 'is': 26, 'long': 27, 'mainly': 28, 'measurements': 29, 'modern': 30, 'multiple': 31, 'observed': 32, 'ocean': 33, 'of': 34, 'periods': 35, 'refers': 36, 'rise': 37, 'shown': 38, 'since': 39, 'system': 40, 'temperature': 41, 'temperatures': 42, 'term': 43, 'the': 44, 'though': 45, 'to': 46, 'warming': 47}\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "gensim_dictionary = corpora.Dictionary(simple_preprocess(sentence, deacc=True) for sentence in open(r'../20 My_Python/Files//text1.txt', encoding='utf-8'))\n",
    "\n",
    "print(gensim_dictionary.token2id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above we read the text file \"file1.txt\" line-by-line using the simple_preprocess method. The method returns tokens in each line of the document. The tokens are then used to create the dictionary. In the output, you should see the tokens and their corresponding IDs.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can create a dictionary by reading multiple text files. Create another file \"file2.txt\" and add the following text to the file (the second part of the first paragraph of the Wikipedia article on Global Warming):\n",
    "\n",
    "In the modern context the terms global warming and climate change are commonly used interchangeably, but climate change includes both global warming and its effects, such as changes to precipitation and impacts that differ by region.[7][8] Many of the observed warming changes since the 1950s are unprecedented in the instrumental temperature record, and in historical and paleoclimate proxy records of climate change over thousands to millions of years.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the \"file2.txt\" in the same directory as the \"file1.txt\".\n",
    "\n",
    "The following script reads both the files and then creates a dictionary based on the text in the two files:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'air': 0, 'also': 1, 'an': 2, 'and': 3, 'aspect': 4, 'average': 5, 'by': 6, 'caused': 7, 'change': 8, 'climate': 9, 'commonly': 10, 'continuing': 11, 'earlier': 12, 'earth': 13, 'economy': 14, 'effects': 15, 'emissions': 16, 'episodes': 17, 'experienced': 18, 'gasses': 19, 'geological': 20, 'global': 21, 'greenhouse': 22, 'in': 23, 'increase': 24, 'industrial': 25, 'is': 26, 'long': 27, 'mainly': 28, 'measurements': 29, 'modern': 30, 'multiple': 31, 'observed': 32, 'ocean': 33, 'of': 34, 'periods': 35, 'refers': 36, 'rise': 37, 'shown': 38, 'since': 39, 'system': 40, 'temperature': 41, 'temperatures': 42, 'term': 43, 'the': 44, 'though': 45, 'to': 46, 'warming': 47, 'are': 48, 'as': 49, 'both': 50, 'but': 51, 'changes': 52, 'context': 53, 'differ': 54, 'historical': 55, 'impacts': 56, 'includes': 57, 'instrumental': 58, 'interchangeably': 59, 'its': 60, 'many': 61, 'millions': 62, 'over': 63, 'paleoclimate': 64, 'precipitation': 65, 'proxy': 66, 'record': 67, 'records': 68, 'region': 69, 'such': 70, 'terms': 71, 'that': 72, 'thousands': 73, 'unprecedented': 74, 'used': 75, 'years': 76}\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "class ReturnTokens(object):\n",
    "    def __init__(self, dir_path):\n",
    "        self.dir_path = dir_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_name in os.listdir(self.dir_path):\n",
    "            for sentence in open(os.path.join(self.dir_path, file_name), encoding='utf-8'):\n",
    "                yield simple_preprocess(sentence)\n",
    "\n",
    "path_to_text_directory = r\"Files\"\n",
    "gensim_dictionary = corpora.Dictionary(ReturnTokens(path_to_text_directory))\n",
    "\n",
    "print(gensim_dictionary.token2id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above we have a method ReturnTokens, which takes the directory path that contains \"file1.txt\" and \"file2.txt\" as the only parameter. Inside the method we iterate through all the files in the directory and then read each file line by line. The simple_preprocess method creates tokens for each line. The tokens for each line are returned to the calling function using the \"yield\" keyword.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Bag of Words Corpus\n",
    "\n",
    "Dictionaries contain mappings between words and their corresponding numeric values. Bag of words corpora in the Gensim library are based on dictionaries and contain the ID of each word along with the frequency of occurrence of the word.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Bag of Words Corpus from In-Memory Objects\n",
    "\n",
    "Look at the following script:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 3), (26, 1), (27, 1), (28, 1), (29, 3), (30, 1), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 2), (44, 2), (45, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "text = [\"\"\"In computer science, artificial intelligence (AI),\n",
    "           sometimes called machine intelligence, is intelligence\n",
    "           demonstrated by machines, in contrast to the natural intelligence\n",
    "           displayed by humans and animals. Computer science defines\n",
    "           AI research as the study of intelligent agents: any device that\n",
    "           perceives its environment and takes actions that maximize its chance\n",
    "           of successfully achieving its goals.\"\"\"]\n",
    "\n",
    "tokens = [[token for token in sentence.split()] for sentence in text]\n",
    "\n",
    "gensim_dictionary = corpora.Dictionary()\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in tokens]\n",
    "\n",
    "print(gensim_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, we have text which we split into tokens. Next, we initialize a Dictionary object from the corpora module. The object contains a method doc2bow, which basically performs two tasks:\n",
    "\n",
    "It iterates through all the words in the text, if the word already exists in the corpus, it increments the frequency count for the word\n",
    "Otherwise it inserts the word into the corpus and sets its frequency count to 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might not make sense to you. Let me explain it. The first tuple (0,1) basically means that the word with ID 0 occurred 1 time in the text. Similarly, (25, 3) means that the word with ID 25 occurred three times in the document.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print the word and the frequency count to make things clear. Add the following lines of code at the end of the previous script:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('(AI),', 1), ('AI', 1), ('Computer', 1), ('In', 1), ('achieving', 1), ('actions', 1), ('agents:', 1), ('and', 2), ('animals.', 1), ('any', 1), ('artificial', 1), ('as', 1), ('by', 2), ('called', 1), ('chance', 1), ('computer', 1), ('contrast', 1), ('defines', 1), ('demonstrated', 1), ('device', 1), ('displayed', 1), ('environment', 1), ('goals.', 1), ('humans', 1), ('in', 1), ('intelligence', 3), ('intelligence,', 1), ('intelligent', 1), ('is', 1), ('its', 3), ('machine', 1), ('machines,', 1), ('maximize', 1), ('natural', 1), ('of', 2), ('perceives', 1), ('research', 1), ('science', 1), ('science,', 1), ('sometimes', 1), ('study', 1), ('successfully', 1), ('takes', 1), ('that', 2), ('the', 2), ('to', 1)]]\n"
     ]
    }
   ],
   "source": [
    "word_frequencies = [[(gensim_dictionary[id], frequence) for id, frequence in couple] for couple in gensim_corpus]\n",
    "print(word_frequencies)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, you can see that the word \"intelligence\" appears three times. Similarly, the word \"that\" appears twice.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Bag of Words Corpus from Text Files\n",
    "\n",
    "Like dictionaries, we can also create a bag of words corpus by reading a text file. Look at the following code:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('air', 1), ('also', 1), ('an', 1), ('and', 3), ('aspect', 1), ('average', 2), ('by', 3), ('caused', 1), ('change', 1), ('climate', 2), ('commonly', 1), ('continuing', 1), ('earlier', 1), ('earth', 1), ('economy', 1), ('effects', 1), ('emissions', 1), ('episodes', 1), ('experienced', 1), ('gasses', 1), ('geological', 1), ('global', 1), ('greenhouse', 1), ('in', 3), ('increase', 1), ('industrial', 1), ('is', 1), ('long', 1), ('mainly', 1), ('measurements', 1), ('modern', 1), ('multiple', 1), ('observed', 1), ('ocean', 1), ('of', 5), ('periods', 1), ('refers', 1), ('rise', 1), ('shown', 1), ('since', 1), ('system', 1), ('temperature', 2), ('temperatures', 1), ('term', 2), ('the', 6), ('though', 1), ('to', 1), ('warming', 3)]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "tokens = [simple_preprocess(sentence, deacc=True) for sentence in open(r'../20 My_Python/Files//text1.txt', encoding='utf-8')]\n",
    "\n",
    "gensim_dictionary = corpora.Dictionary()\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in tokens]\n",
    "word_frequencies = [[(gensim_dictionary[id], frequence) for id, frequence in couple] for couple in gensim_corpus]\n",
    "\n",
    "print(word_frequencies)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, we created a bag of words corpus using \"file1.txt\". In the output, you should see the words in the first paragraph for the Global Warming article on Wikipedia.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output, shows that the words like \"of\", \"the\", \"by\", and \"and\" occur twice.\n",
    "\n",
    "Similarly, you can create a bag of words corpus using multiple text files, as shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('air', 1), ('also', 1), ('an', 1), ('and', 3), ('aspect', 1), ('average', 2), ('by', 3), ('caused', 1), ('change', 1), ('climate', 2), ('commonly', 1), ('continuing', 1), ('earlier', 1), ('earth', 1), ('economy', 1), ('effects', 1), ('emissions', 1), ('episodes', 1), ('experienced', 1), ('gasses', 1), ('geological', 1), ('global', 1), ('greenhouse', 1), ('in', 3), ('increase', 1), ('industrial', 1), ('is', 1), ('long', 1), ('mainly', 1), ('measurements', 1), ('modern', 1), ('multiple', 1), ('observed', 1), ('ocean', 1), ('of', 5), ('periods', 1), ('refers', 1), ('rise', 1), ('shown', 1), ('since', 1), ('system', 1), ('temperature', 2), ('temperatures', 1), ('term', 2), ('the', 6), ('though', 1), ('to', 1), ('warming', 3)], [('and', 5), ('by', 1), ('change', 3), ('climate', 3), ('commonly', 1), ('effects', 1), ('global', 2), ('in', 3), ('modern', 1), ('observed', 1), ('of', 3), ('since', 1), ('temperature', 1), ('the', 5), ('to', 2), ('warming', 3), ('are', 2), ('as', 1), ('both', 1), ('but', 1), ('changes', 2), ('context', 1), ('differ', 1), ('historical', 1), ('impacts', 1), ('includes', 1), ('instrumental', 1), ('interchangeably', 1), ('its', 1), ('many', 1), ('millions', 1), ('over', 1), ('paleoclimate', 1), ('precipitation', 1), ('proxy', 1), ('record', 1), ('records', 1), ('region', 1), ('such', 1), ('terms', 1), ('that', 1), ('thousands', 1), ('unprecedented', 1), ('used', 1), ('years', 1)]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "class ReturnTokens(object):\n",
    "    def __init__(self, dir_path):\n",
    "        self.dir_path = dir_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_name in os.listdir(self.dir_path):\n",
    "            for sentence in open(os.path.join(self.dir_path, file_name), encoding='utf-8'):\n",
    "                yield simple_preprocess(sentence)\n",
    "\n",
    "path_to_text_directory = r\"Files\"\n",
    "\n",
    "gensim_dictionary = corpora.Dictionary()\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in ReturnTokens(path_to_text_directory)]\n",
    "word_frequencies = [[(gensim_dictionary[id], frequence) for id, frequence in couple] for couple in gensim_corpus]\n",
    "\n",
    "print(word_frequencies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating TF-IDF Corpus\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words approach works fine for converting text to numbers. However, it has one drawback. It assigns a score to a word based on its occurrence in a particular document. It doesn't take into account the fact that the word might also have a high frequency of occurrences in other documents as well. TF-IDF resolves this issue.\n",
    "\n",
    "The term frequency is calculated as:\n",
    "\n",
    "Term frequency = (Frequency of the word in a document)/(Total words in the document)\n",
    "\n",
    "And the Inverse Document Frequency is calculated as:\n",
    "\n",
    "IDF(word) = Log((Total number of documents)/(Number of documents containing the word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using the Gensim library, we can easily create a TF-IDF corpus:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Football', 0.35], ['I', 0.71], ['like', 0.35], ['play', 0.35], ['to', 0.35]]\n",
      "[['Football', 0.27], ['best', 0.53], ['game', 0.27], ['is', 0.53], ['the', 0.53]]\n",
      "[['like', 0.22], ['play', 0.22], ['to', 0.22], ['game', 0.22], ['?', 0.45], ['Which', 0.45], ['do', 0.45], ['you', 0.45]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "text = [\"I like to play Football\",\n",
    "       \"Football is the best game\",\n",
    "       \"Which game do you like to play ?\"]\n",
    "\n",
    "tokens = [[token for token in sentence.split()] for sentence in text]\n",
    "\n",
    "gensim_dictionary = corpora.Dictionary()\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in tokens]\n",
    "\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "tfidf = models.TfidfModel(gensim_corpus, smartirs='ntc')\n",
    "\n",
    "for sent in tfidf[gensim_corpus]:\n",
    "    print([[gensim_dictionary[id], np.around(frequency, decimals=2)] for id, frequency in sent])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the TF-IDF value, we can use the TfidfModel class from the models module of the Gensim library. We simply have to pass the bag of word corpus as a parameter to the constructor of the TfidfModel class. In the output, you will see all of the words in the three sentences, along with their TF-IDF values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Built-In Gensim Models and Datasets\n",
    "\n",
    "Gensim comes with a variety of built-in datasets and word embedding models that can be directly used.\n",
    "\n",
    "To download a built-in model or dataset, we can use the downloader class from the gensim library. We can then call the load method on the downloader class to download the desired package. Look at the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_embedding = api.load(\"glove-wiki-gigaword-100\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the commands above, we download the \"glove-wiki-gigaword-100\" word embedding model, which is basically based on Wikipedia text and is 100 dimensional. Let's try to find the words similar to \"toyota\" using our word embedding model. Use the following code to do so:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('honda', 0.8739858269691467),\n",
       " ('nissan', 0.8108118176460266),\n",
       " ('automaker', 0.7918164730072021),\n",
       " ('mazda', 0.7687168121337891),\n",
       " ('bmw', 0.7616022229194641),\n",
       " ('ford', 0.7547588348388672),\n",
       " ('motors', 0.7539199590682983),\n",
       " ('volkswagen', 0.7176680564880371),\n",
       " ('prius', 0.7156581878662109),\n",
       " ('chrysler', 0.7085398435592651)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_embedding.most_similar('toyota')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see all the results are very relevant to the word \"toyota\". The number in the fraction corresponds to the similarity index. Higher similarity index means that the word is more relevant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
